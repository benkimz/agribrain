{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkhsesctnWmw",
        "outputId": "5c20be02-dd80-46e4-fdf1-2888b0ff68e1"
      },
      "id": "JkhsesctnWmw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6ZUSl4uxUeW",
        "outputId": "f4263214-6a83-4e69-9382-1595a656700e"
      },
      "id": "B6ZUSl4uxUeW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acda78f4-5ba8-4f05-9bfd-f5f0de0f58be",
      "metadata": {
        "id": "acda78f4-5ba8-4f05-9bfd-f5f0de0f58be"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import json\n",
        "import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxZqb96JnblH"
      },
      "id": "ZxZqb96JnblH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./drive/MyDrive/agribrain/corpus/batch.zip -d ./corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0nNMo0Mnbr1",
        "outputId": "511e1434-65ad-4f0a-acfe-0efecd367cae"
      },
      "id": "j0nNMo0Mnbr1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./drive/MyDrive/agribrain/corpus/batch.zip\n",
            "  inflating: ./corpus/batch-01.tar.gz  \n",
            "  inflating: ./corpus/batch-02.tar.gz  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "with tarfile.open(\"./corpus/batch-01.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall(\"./corpus/\")\n",
        "\n",
        "with tarfile.open(\"./corpus/batch-02.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall(\"./corpus/\")    "
      ],
      "metadata": {
        "id": "AWIJSZEeoBc1"
      },
      "id": "AWIJSZEeoBc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e6f941-005d-4bfe-b06f-c004151dfd7d",
      "metadata": {
        "id": "66e6f941-005d-4bfe-b06f-c004151dfd7d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791a9fc6-5605-4b19-8523-6d25ff02b5c0",
      "metadata": {
        "id": "791a9fc6-5605-4b19-8523-6d25ff02b5c0"
      },
      "outputs": [],
      "source": [
        "class TextFilesParser:\n",
        "    def __init__(self, source_dir=None, output_dir=None, extensions=[\".txt\"]):\n",
        "        output_dir = output_dir if output_dir else \"./txt-parser-ouputs/\"\n",
        "        assert type(output_dir) == str\n",
        "        output_dir = re.sub(r\"\\s+\", \"_\", output_dir)\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        if source_dir:\n",
        "            for file in os.listdir(source_dir):\n",
        "                file_path = os.path.join(source_dir, os.path.basename(file))\n",
        "                if not os.path.isfile(file_path): continue\n",
        "                file_name, fext = os.path.splitext(file)\n",
        "                if not fext in extensions: continue\n",
        "                output_file = os.path.join(output_dir, file_name + fext)\n",
        "                with open(file_path, mode=\"r\", encoding=\"utf-8\") as context:\n",
        "                    content = context.read()\n",
        "                    \n",
        "                parser_content = self.paragraph_parser(content)\n",
        "                if not parser_content is None and type(parser_content) == str:\n",
        "                    with open(output_file, mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "                        fp.write(parser_content)\n",
        "                        fp.close()\n",
        "        return None\n",
        "    \n",
        "    def line_parser(self, line):\n",
        "        line = re.sub(r\"\\s+\", \" \", line)\n",
        "        line = re.sub(r\"\\t+\", \" \", line)\n",
        "        line = re.sub(r\"www?(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", '[URL_ADDRESS]', line)\n",
        "        line = re.sub(r'^[\\w-]+(\\.[\\w-]+)*@[\\w-]+(\\.[\\w-]+)*(\\.[a-zA-Z]{2,})$', \n",
        "                      '[EMAIL_ADDRESS]', line)\n",
        "        if len(line.split(\"@\")) >=2: return None\n",
        "        if line and not line == \" \":\n",
        "            return line\n",
        "        \n",
        "    def paragraph_parser(self, corpus):\n",
        "        # removing any non utf-8 characters\n",
        "        non_utf8 = pattern = re.compile(r'[^\\x00-\\x7F]')\n",
        "        corpus = non_utf8.sub(\"\", corpus)\n",
        "\n",
        "        # split the corpus into paragraphs\n",
        "        paragraphs = corpus.split(\"\\n\\n\")\n",
        "        new_paragraphs = []\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph = paragraph.strip()\n",
        "            sents = [self.line_parser(line) for line in paragraph.split(\"\\n\") if self.line_parser(line)]       \n",
        "            \n",
        "            num_sents = len(sents)\n",
        "            if not num_sents > 8:\n",
        "                continue\n",
        "\n",
        "            average_num_words_track = []\n",
        "            for sent in sents:\n",
        "                average_num_words_track.append(len(sent.split()))\n",
        "                \n",
        "            # Average words in a sentence\n",
        "            average_words = np.mean(np.array(average_num_words_track))\n",
        "            \n",
        "            # Average symbols stops in a sentence\n",
        "            symbols_list = [\".\", \",\", \":\", \";\"]\n",
        "            average_symbols_count = 0\n",
        "            for symb in symbols_list:\n",
        "                average_symbols_count += len(paragraph.split(symb)) / num_sents\n",
        "            if average_symbols_count > 0:\n",
        "                if (average_symbols_count / len(symbols_list)) >= 3:\n",
        "                    continue\n",
        "                    \n",
        "            if not int(average_words) >= 8:\n",
        "                continue\n",
        "            paragraph = \"\".join(sents)\n",
        "            paragraph = re.sub(r\"www?(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", \n",
        "                               '[URL_ADDRESS]', paragraph)\n",
        "            paragraph = re.sub(r'^[\\w-]+(\\.[\\w-]+)*@[\\w-]+(\\.[\\w-]+)*(\\.[a-zA-Z]{2,})$', \n",
        "                          '[EMAIL_ADDRESS]', paragraph)            \n",
        "            new_paragraphs.append(paragraph)\n",
        "        if len(new_paragraphs) > 0:\n",
        "            return \"\\n\\n\".join(new_paragraphs)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8edb5096-ee38-43b0-b2cb-312926e2b20a",
      "metadata": {
        "id": "8edb5096-ee38-43b0-b2cb-312926e2b20a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc84104-9bf5-409c-9f37-3735a1c456d3",
      "metadata": {
        "id": "9fc84104-9bf5-409c-9f37-3735a1c456d3"
      },
      "outputs": [],
      "source": [
        "class TextFileClusters(KMeans):\n",
        "    def __init__(self, source_dir=None, output_dir=None, n_clusters=5, random_state=42, max_iter=1000000):\n",
        "        \n",
        "        output_dir = output_dir if output_dir else \"./txt-cluster-output/\"\n",
        "        assert type(output_dir) == str\n",
        "        self.output_dir = re.sub(r\"\\s+\", \"_\", output_dir)\n",
        "        \n",
        "        self.n_clusters = n_clusters\n",
        "        self.random_state = random_state\n",
        "        self.max_iter = max_iter\n",
        "        self.source_dir = source_dir\n",
        "        \n",
        "        assert os.path.exists(self.source_dir), \"self.source_dir not found\"\n",
        "        \n",
        "        self.files_kmeans = KMeans(n_clusters=self.n_clusters, \n",
        "                                   random_state=self.random_state, \n",
        "                                   n_init=\"auto\", \n",
        "                                   max_iter=self.max_iter)\n",
        "        \n",
        "        self.files_text_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "        \n",
        "        data = self._get_text_data()\n",
        "        lbls = self._kmeans_clustering(data) ###\n",
        "        self._save_clusters(lbls)\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def save_components(self, save_dir: str, overwrite_dir=False):\n",
        "        if os.path.exists(save_dir) and not overwrite_dir:\n",
        "            raise Exception(\"\"\"\n",
        "            K-model: {} exists. Select a different name or set to overwrite contents.\n",
        "            \"\"\".format(save_dir))\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        model_path = os.path.join(save_dir, \"kmeans-model.sav\")\n",
        "        vectorizer_path = os.path.join(save_dir, \"text-vectorizer.sav\")\n",
        "        pickle.dump(self.files_kmeans, open(model_path, mode=\"wb\"))\n",
        "        pickle.dump(self.files_text_vectorizer, open(vectorizer_path, mode=\"wb\"))\n",
        "        print(\"Model, {} saved successfully.\".format(save_dir))\n",
        "    \n",
        "    def _get_text_data(self):\n",
        "        textdata = []\n",
        "        for filepath in glob.glob(os.path.join(self.source_dir, \"*.txt\")):\n",
        "            with open(filepath, mode=\"r\", encoding=\"utf-8\") as context:\n",
        "                textdata.append(context.read())\n",
        "        return textdata\n",
        "    \n",
        "    def _kmeans_clustering(self, text_data):\n",
        "        # Perform k-means clustering on text data\n",
        "        inputs = self.files_text_vectorizer.fit_transform(text_data)\n",
        "        \n",
        "        self.files_kmeans.fit(inputs)\n",
        "        \n",
        "        cluster_labels = self.files_kmeans.labels_\n",
        "        \n",
        "        return cluster_labels    \n",
        "    \n",
        "    def _save_clusters(self, cluster_labels):\n",
        "        # Copy files to cluster directories\n",
        "        if not os.path.exists(self.output_dir):\n",
        "            os.makedirs(self.output_dir)\n",
        "        for i in range(max(cluster_labels) + 1):            \n",
        "            cluster_dir = os.path.join(self.output_dir, f\"cluster-{i}\")\n",
        "            if not os.path.exists(cluster_dir):\n",
        "                os.mkdir(cluster_dir)\n",
        "            for j, filename in enumerate(glob.glob(os.path.join(self.source_dir, '*.txt'))):\n",
        "                if cluster_labels[j] == i:\n",
        "                    outfile = os.path.join(cluster_dir, os.path.basename(filename))\n",
        "                    with open(filename, mode=\"r\", encoding=\"utf-8\") as readctx, \\\n",
        "                    open(outfile, mode=\"w\", encoding=\"utf-8\") as writectx:\n",
        "                        writectx.write(readctx.read())  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hb7OwfP2peJg"
      },
      "id": "Hb7OwfP2peJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the text files\n",
        "TextFilesParser(source_dir=\"./corpus/batch-01/\", output_dir=\"./drive/MyDrive/agribrain/processed-corpus/batch-01/\")\n",
        "TextFilesParser(source_dir=\"./corpus/batch-02/\", output_dir=\"./drive/MyDrive/agribrain/processed-corpus/batch-02/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKR5mAV9peNY",
        "outputId": "ec57464f-c56c-4b13-c981-6978262876de"
      },
      "id": "jKR5mAV9peNY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.TextFilesParser at 0x7f73139b5850>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0AM0uJAfpeR1"
      },
      "id": "0AM0uJAfpeR1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./drive/MyDrive/agribrain/processed-corpus-collection/\n",
        "!cp ./drive/MyDrive/agribrain/processed-corpus/batch-01/*.txt ./drive/MyDrive/agribrain/processed-corpus-collection/\n",
        "!cp ./drive/MyDrive/agribrain/processed-corpus/batch-02/*.txt ./drive/MyDrive/agribrain/processed-corpus-collection/"
      ],
      "metadata": {
        "id": "kCTujupUp9Uz"
      },
      "id": "kCTujupUp9Uz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFcp1PSxp9j8"
      },
      "id": "uFcp1PSxp9j8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster the text files\n",
        "kmodel = TextFileClusters(\"./drive/MyDrive/agribrain/processed-corpus-collection/\", \n",
        "                          \"./drive/MyDrive/agribrain/clustered-corpus/\", \n",
        "                          random_state=42, \n",
        "                          max_iter=10000000, \n",
        "                          n_clusters=128)"
      ],
      "metadata": {
        "id": "2o1ADuY2pwbv"
      },
      "id": "2o1ADuY2pwbv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d09516-0a23-4116-a69d-6291d28d21dc",
      "metadata": {
        "id": "a7d09516-0a23-4116-a69d-6291d28d21dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c390cf99-ff49-4e24-b0ae-b099bdac7ce6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c390cf99-ff49-4e24-b0ae-b099bdac7ce6",
        "outputId": "722658f0-c3d4-4802-f3de-ff4a4a732b43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "kmodel.files_kmeans.predict(\n",
        "    kmodel.files_text_vectorizer.transform([\"Tea farming\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6415a5c2-a7e3-4a71-9899-806c74a3ddbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6415a5c2-a7e3-4a71-9899-806c74a3ddbe",
        "outputId": "bfedb147-39c4-4ea9-9b11-3e87a7663b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model, ./drive/MyDrive/agribrain/kmodels/K-files-cluster/ saved successfully.\n"
          ]
        }
      ],
      "source": [
        "kmodel.save_components(\"./drive/MyDrive/agribrain/kmodels/K-files-cluster/\", overwrite_dir=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a831e8ef-6128-40c4-932c-808758d2ac10",
      "metadata": {
        "id": "a831e8ef-6128-40c4-932c-808758d2ac10"
      },
      "outputs": [],
      "source": [
        "def load_kmodel(model_name):\n",
        "    assert os.path.exists(model_name), \"Model does not exist\"\n",
        "    model_path = os.path.join(model_name, \"kmeans-model.sav\")\n",
        "    kmeans_model = pickle.load(open(model_path, mode=\"rb\"))\n",
        "    return kmeans_model\n",
        "\n",
        "def load_kvec(model_name):\n",
        "    assert os.path.exists(model_name), \"Model does not exist\"\n",
        "    vectorizer_path = os.path.join(model_name, \"text-vectorizer.sav\")\n",
        "    vectorizer = pickle.load(open(vectorizer_path, mode=\"rb\"))\n",
        "    return vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b950566-9308-491c-8add-4cf80c9aedd8",
      "metadata": {
        "id": "6b950566-9308-491c-8add-4cf80c9aedd8"
      },
      "outputs": [],
      "source": [
        "vectorizer = load_kvec(\"./drive/MyDrive/agribrain/kmodels/K-files-cluster/\")\n",
        "model = load_kmodel(\"./drive/MyDrive/agribrain/kmodels/K-files-cluster/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef42fff5-061d-4b26-bc1f-1ff1efd9ecc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef42fff5-061d-4b26-bc1f-1ff1efd9ecc0",
        "outputId": "265d9002-6942-4e11-a7c5-250556fb14db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.predict(vectorizer.transform([\"Tea farming\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10b3d16-df66-44aa-b4dd-30cc56bda5a2",
      "metadata": {
        "id": "c10b3d16-df66-44aa-b4dd-30cc56bda5a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3763e6dd-111b-4b3d-a5ce-91e2703e777d",
      "metadata": {
        "id": "3763e6dd-111b-4b3d-a5ce-91e2703e777d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b727b366-2196-4174-9ce4-886a1adff8b5",
      "metadata": {
        "id": "b727b366-2196-4174-9ce4-886a1adff8b5"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"philschmid/bart-large-cnn-samsum\")"
      ],
      "metadata": {
        "id": "ftD-9i9VxShz"
      },
      "id": "ftD-9i9VxShz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5601294e-6f8c-429a-b68a-bfb9b24b6055",
      "metadata": {
        "id": "5601294e-6f8c-429a-b68a-bfb9b24b6055"
      },
      "outputs": [],
      "source": [
        "def generate_summary(input_text):\n",
        "  input_ids = tokenizer.encode(input_text, max_length=1024, truncation=True, return_tensors='pt')\n",
        "\n",
        "  # Generate the summary\n",
        "  summary_ids = model.generate(input_ids, max_length=480, min_length=10, num_beams=4, early_stopping=True)\n",
        "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./drive/MyDrive/agribrain/processed-corpus-summary/"
      ],
      "metadata": {
        "id": "ZEi4U0oRx78P"
      },
      "id": "ZEi4U0oRx78P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dest_dir = \"./drive/MyDrive/agribrain/processed-corpus-summary/\"\n",
        "\n",
        "def mass_corpus_summarization(source_dir, dest_dir):\n",
        "  if not os.path.exists(dest_dir):\n",
        "    os.makedirs(dest_dir)\n",
        "  for filename in os.listdir(source_dir):\n",
        "    file_path = os.path.join(source_dir, filename)\n",
        "    fn, fe = os.path.splitext(filename)\n",
        "    if not fe == \".txt\":\n",
        "      continue\n",
        "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as rctx:\n",
        "      content = rctx.read()\n",
        "\n",
        "    summary_file = os.path.join(dest_dir, filename)\n",
        "    with open(summary_file, mode=\"w\", encoding=\"utf-8\") as wctx:\n",
        "      wctx.write(generate_summary(content))"
      ],
      "metadata": {
        "id": "1YVZOURnx8At"
      },
      "id": "1YVZOURnx8At",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text = \"\"\"\n",
        "  This is some sample text that I want to summarize. \n",
        "  It is not very long, but it should be enough to test the function.\n",
        "\"\"\"\n",
        "summary = generate_summary(testing_text)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGTq795-xv4N",
        "outputId": "a9dbc237-4842-448d-a92e-1fb93a77078f"
      },
      "id": "SGTq795-xv4N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sample text is enough to test the function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6atzTXoF4O1"
      },
      "id": "T6atzTXoF4O1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}