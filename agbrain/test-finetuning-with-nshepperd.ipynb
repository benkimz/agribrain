{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpIuEK7tIVOA",
        "outputId": "0e991936-9c94-4b0a-9a16-f51dfc84f436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFE72oaBTWKA",
        "outputId": "aa744265-3ae2-4c6a-93bb-ebac7e5d9a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 429, done.\u001b[K\n",
            "remote: Total 429 (delta 0), reused 0 (delta 0), pack-reused 429\u001b[K\n",
            "Receiving objects: 100% (429/429), 4.47 MiB | 9.32 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFmmL-3vUSxT",
        "outputId": "1a0d06a5-1ad0-45fe-c9ff-a5a85714241e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gpt-2  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./gpt-2/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UCTdBnRUVMq",
        "outputId": "2f2452fd-3569-4834-ad79-a74445548473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-2\n",
            "CONTRIBUTORS.md  Dockerfile.gpu     encode.py\t   README.md\t     train.py\n",
            "DEVELOPERS.md\t domains.txt\t    LICENSE\t   requirements.txt  twremat\n",
            "Dockerfile.cpu\t download_model.py  model_card.md  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHA-YnwAUZGx",
        "outputId": "9c24f65c-f360-4eba-b0d8-479bde8b3cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fire>=0.1.3\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting regex==2017.4.5\n",
            "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.6/601.6 KB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests==2.21.0\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 KB\u001b[0m \u001b[31m635.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "  Downloading tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2022.12.7)\n",
            "Collecting urllib3<1.25,>=1.21.1\n",
            "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (2.2.0)\n",
            "Building wheels for collected packages: regex, fire\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp39-cp39-linux_x86_64.whl size=677630 sha256=185cfdc5196797b625138036d3251b74e7e3ddc7f1937b28919a8dcb61cfd89d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/70/3e/734b51125cb2502e07f14b79b8aa2f06ade3dcee9d0e9c79cc\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=d2d319ba763e950558587ff0479b9e5b3db99271e6252e63189b99eaa5e2e002\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built regex fire\n",
            "Installing collected packages: toposort, regex, urllib3, tqdm, idna, fire, requests\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.10.31\n",
            "    Uninstalling regex-2022.10.31:\n",
            "      Successfully uninstalled regex-2022.10.31\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.13 requires requests>=2.26, but you have requests 2.21.0 which is incompatible.\n",
            "tweepy 4.13.0 requires requests<3,>=2.27.0, but you have requests 2.21.0 which is incompatible.\n",
            "spacy 3.5.1 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "prophet 1.1.2 requires tqdm>=4.36.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "panel 0.14.4 requires tqdm>=4.48.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires requests>=2.24.0, but you have requests 2.21.0 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires tqdm>=4.48.2, but you have tqdm 4.31.1 which is incompatible.\n",
            "nltk 3.8.1 requires regex>=2021.8.3, but you have regex 2017.4.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests>=2.27.0, but you have requests 2.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fire-0.5.0 idna-2.8 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1 urllib3-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ../drive/MyDrive/agribrain/corpus/corpus.txt . "
      ],
      "metadata": {
        "id": "SVKi2d-KUzXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDdr0Bh6VDl5",
        "outputId": "0c62d975-8d06-4674-d988-eb06d62bafcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONTRIBUTORS.md  Dockerfile.cpu  download_model.py  model_card.md     src\n",
            "corpus.txt\t Dockerfile.gpu  encode.py\t    README.md\t      train.py\n",
            "DEVELOPERS.md\t domains.txt\t LICENSE\t    requirements.txt  twremat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 download_model.py 124M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fN1Dq9PUme8",
        "outputId": "f9d45e8f-9712-4f7b-ab98-fe5da3db72f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 884kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:01, 558kit/s]                                                    \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.03Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:08, 7.27Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.65Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:01, 373kit/s]                                                  \n",
            "Fetching vocab.bpe: 457kit [00:01, 367kit/s]                                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZYsRvzYVzHk",
        "outputId": "dab2cdd6-5202-48e6-8768-8b93ed253b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONTRIBUTORS.md  Dockerfile.gpu     LICENSE\t   requirements.txt\n",
            "corpus.txt\t domains.txt\t    model_card.md  src\n",
            "DEVELOPERS.md\t download_model.py  models\t   train.py\n",
            "Dockerfile.cpu\t encode.py\t    README.md\t   twremat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv encode.py ./src/\n",
        "!mv train.py ./src/"
      ],
      "metadata": {
        "id": "HOpJvKJVWXpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./src/encode.py corpus.txt ./src/corpus.npz --model_name 124M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MV5HgNOWEWF",
        "outputId": "8948f5fe-b387-4b0b-f53a-3bdf009ee2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-27 06:17:12.551227: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-27 06:17:13.419548: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-27 06:17:13.419645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-27 06:17:13.419663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Reading files\n",
            "100% 1/1 [01:23<00:00, 83.12s/it]\n",
            "Writing ./src/corpus.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./src/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHFTya2ba9L9",
        "outputId": "48ce997e-e24f-46f7-e8e2-f79c3897ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-2/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ../models . "
      ],
      "metadata": {
        "id": "B58qVjc4buSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch\n",
        "!pip install -q --upgrade tqdm\n",
        "!pip install -q --upgrade requests\n",
        "!pip install -q --upgrade regex\n",
        "!pip install -q --upgrade transformers\n",
        "!pip install -q --upgrade tensorflow"
      ],
      "metadata": {
        "id": "EJmV2XuttBf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --dataset corpus.npz --model_name 124M --run_name agbrain --batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRqvlxyxWSck",
        "outputId": "76bcfc59-8ec2-4ca6-e9ec-f1fe29a81c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-27 06:21:55.853394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-27 06:21:57.539500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-27 06:22:00.839815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:01.461869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:01.462216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:01.462948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:01.463200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:01.463395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:02.581833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:02.582145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:02.582359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-27 06:22:02.582501: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-03-27 06:22:02.582541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13678 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "Using Adam optimizer\n",
            "2023-03-27 06:22:11.624130: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "2023-03-27 06:22:12.157294: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "2023-03-27 06:22:12.224998: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "2023-03-27 06:22:12.926744: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "2023-03-27 06:22:13.032350: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "2023-03-27 06:22:14.140021: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "dataset has 22537630 tokens\n",
            "Training...\n",
            "[1 | 4.21] loss=4.39 avg=4.39\n",
            "[2 | 5.01] loss=4.45 avg=4.42\n",
            "[3 | 5.82] loss=4.62 avg=4.49\n",
            "[4 | 6.64] loss=4.36 avg=4.46\n",
            "[5 | 7.45] loss=4.42 avg=4.45\n",
            "[6 | 8.27] loss=4.07 avg=4.38\n",
            "[7 | 9.08] loss=4.62 avg=4.42\n",
            "[8 | 9.90] loss=4.01 avg=4.37\n",
            "[9 | 10.71] loss=2.70 avg=4.17\n",
            "[10 | 11.53] loss=4.41 avg=4.20\n",
            "[11 | 12.35] loss=2.91 avg=4.07\n",
            "[12 | 13.17] loss=4.39 avg=4.10\n",
            "[13 | 13.99] loss=4.63 avg=4.15\n",
            "[14 | 14.81] loss=3.84 avg=4.12\n",
            "[15 | 15.62] loss=4.46 avg=4.15\n",
            "[16 | 16.45] loss=4.18 avg=4.15\n",
            "[17 | 17.27] loss=2.79 avg=4.06\n",
            "[18 | 18.10] loss=4.19 avg=4.07\n",
            "[19 | 18.92] loss=3.91 avg=4.06\n",
            "[20 | 19.75] loss=4.26 avg=4.07\n",
            "[21 | 20.57] loss=4.43 avg=4.09\n",
            "[22 | 21.40] loss=4.36 avg=4.10\n",
            "[23 | 22.23] loss=3.96 avg=4.10\n",
            "[24 | 23.06] loss=4.03 avg=4.09\n",
            "[25 | 23.89] loss=2.81 avg=4.04\n",
            "[26 | 24.73] loss=4.07 avg=4.04\n",
            "[27 | 25.56] loss=4.25 avg=4.05\n",
            "[28 | 26.39] loss=4.18 avg=4.05\n",
            "[29 | 27.22] loss=4.22 avg=4.06\n",
            "[30 | 28.06] loss=4.38 avg=4.07\n",
            "[31 | 28.89] loss=2.91 avg=4.03\n",
            "[32 | 29.72] loss=4.39 avg=4.04\n",
            "[33 | 30.56] loss=3.57 avg=4.02\n",
            "[34 | 31.40] loss=2.44 avg=3.97\n",
            "[35 | 32.23] loss=4.08 avg=3.97\n",
            "[36 | 33.07] loss=2.75 avg=3.93\n",
            "[37 | 33.91] loss=3.93 avg=3.93\n",
            "[38 | 34.76] loss=4.26 avg=3.94\n",
            "[39 | 35.60] loss=4.24 avg=3.95\n",
            "[40 | 36.44] loss=2.80 avg=3.92\n",
            "[41 | 37.29] loss=2.71 avg=3.88\n",
            "[42 | 38.13] loss=4.38 avg=3.90\n",
            "[43 | 38.98] loss=3.91 avg=3.90\n",
            "[44 | 39.82] loss=2.83 avg=3.87\n",
            "[45 | 40.68] loss=2.70 avg=3.83\n",
            "[46 | 41.53] loss=4.56 avg=3.85\n",
            "[47 | 42.38] loss=2.81 avg=3.83\n",
            "[48 | 43.23] loss=4.14 avg=3.83\n",
            "[49 | 44.09] loss=4.08 avg=3.84\n",
            "[50 | 44.94] loss=4.03 avg=3.85\n",
            "[51 | 45.79] loss=2.58 avg=3.81\n",
            "[52 | 46.64] loss=4.41 avg=3.83\n",
            "[53 | 47.50] loss=4.43 avg=3.84\n",
            "[54 | 48.36] loss=4.26 avg=3.85\n",
            "[55 | 49.22] loss=3.78 avg=3.85\n",
            "[56 | 50.07] loss=4.17 avg=3.86\n",
            "[57 | 50.94] loss=3.93 avg=3.86\n",
            "[58 | 51.80] loss=2.66 avg=3.83\n",
            "[59 | 52.66] loss=4.29 avg=3.84\n",
            "[60 | 53.53] loss=4.18 avg=3.85\n",
            "[61 | 54.40] loss=4.02 avg=3.85\n",
            "[62 | 55.26] loss=4.14 avg=3.86\n",
            "[63 | 56.13] loss=4.09 avg=3.87\n",
            "[64 | 57.00] loss=2.37 avg=3.83\n",
            "[65 | 57.88] loss=2.55 avg=3.81\n",
            "[66 | 58.75] loss=4.45 avg=3.82\n",
            "[67 | 59.62] loss=2.69 avg=3.80\n",
            "[68 | 60.50] loss=2.49 avg=3.77\n",
            "[69 | 61.38] loss=4.02 avg=3.78\n",
            "[70 | 62.25] loss=3.91 avg=3.78\n",
            "[71 | 63.13] loss=4.85 avg=3.80\n",
            "[72 | 64.01] loss=4.26 avg=3.81\n",
            "[73 | 64.89] loss=2.88 avg=3.79\n",
            "[74 | 65.77] loss=2.67 avg=3.77\n",
            "[75 | 66.65] loss=2.70 avg=3.75\n",
            "[76 | 67.52] loss=2.76 avg=3.73\n",
            "[77 | 68.41] loss=4.00 avg=3.74\n",
            "[78 | 69.29] loss=4.53 avg=3.75\n",
            "[79 | 70.17] loss=2.55 avg=3.73\n",
            "[80 | 71.06] loss=4.00 avg=3.73\n",
            "[81 | 71.95] loss=4.40 avg=3.75\n",
            "[82 | 72.84] loss=4.10 avg=3.75\n",
            "[83 | 73.73] loss=2.62 avg=3.73\n",
            "[84 | 74.63] loss=2.72 avg=3.71\n",
            "[85 | 75.53] loss=3.85 avg=3.72\n",
            "[86 | 76.42] loss=4.22 avg=3.73\n",
            "[87 | 77.32] loss=4.19 avg=3.73\n",
            "[88 | 78.22] loss=2.96 avg=3.72\n",
            "[89 | 79.12] loss=3.82 avg=3.72\n",
            "[90 | 80.03] loss=2.72 avg=3.70\n",
            "[91 | 80.93] loss=4.60 avg=3.72\n",
            "[92 | 81.84] loss=4.21 avg=3.73\n",
            "[93 | 82.75] loss=3.55 avg=3.72\n",
            "[94 | 83.66] loss=3.80 avg=3.73\n",
            "[95 | 84.58] loss=2.76 avg=3.71\n",
            "[96 | 85.49] loss=3.77 avg=3.71\n",
            "[97 | 86.41] loss=4.10 avg=3.72\n",
            "[98 | 87.33] loss=4.07 avg=3.72\n",
            "[99 | 88.25] loss=4.02 avg=3.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " on the other hand, you could easily buy a new, more durable piece of equipment in every neighborhood.\n",
            "At the same time, you could take care of your family.\n",
            "There are a number of reasons where housing is your responsibility (i.e., your livelihood, your children's).\n",
            "To get it started, if you have more than three children, you should have a reliable and flexible means of income (which includes transportation). If you have only one or two children, you need to rely on a mortgage. It is an income, not a fixed income.\n",
            "Having a place to stay (or a reliable transportation connection) depends on your family size, needs, and financial ability. It is very important that your family does not have to go outside.\n",
            "At the same time, housing is important to maintain family income. If your family is able to move on, it is an investment in your future. Housing is not a permanent income like land. Housing is simply a piece of money that is used to buy a house.\n",
            "When deciding how much housing should be rent    , make sure that you include these requirements into your income calculation when planning your rent.\n",
            "If you pay a fixed rate, then the rent is just a portion of the rent you receive from your property. In that case, you have a fixed rate (that means, depending on the level of income you live).\n",
            "If you do pay rent, then you have to pay additional funds that should be used to buy the property.\n",
            "As with all rent obligations, be sure to include these requirements when selecting your rent.\n",
            "If you have income that is not included in your rent (if it is still over- or below-targeted) then if your income is a reasonable, stable rental income, then consider paying rent at this time. If it is more than targeted, then consider a different income.\n",
            "In case of household income below target, then, you need to factor income into your income calculation if your income is below-targeted.\n",
            "If you have earned an income that makes a reasonable amount, then you need to factor income into income calculation, or you will be considered an income poor .\n",
            "In case any of these costs of living you have to assume that it will pay rent to someone within, or near, your household, then consider determining that, if you have income of at least 50% of your income is considered sufficient income to fund rent.\n",
            "This example, from your household income below target, might not be true; it would be true if someone lived below 50% of your income, because they must pay rent.\n",
            "However, if you live in your house, you would have received income that would allow them to repay this loan or borrow money. This example, from my household income below target, could potentially be true. However, it would be true if you received a loan or loan from someone, or if someone lived within your house, you also received income that would allow them to repay this loan. This example is taken from my household income below target because, it would include income that would allow them to repay this loan or borrow money.\n",
            "If you earn this amount, then rent is an expense so pay rent to someone within your household.\n",
            "However, this example would be true if it included income that would allow you to repay this loan or borrow money to pay rent to someone within your house. (Note: there is another example of income that would allow you to repay this loan or borrow money to pay rent in case of an emergency).\n",
            "In case of household income below target, then, if someone lives in their house, rent is an expense so pay rent to someone within your household. However, this example (from my household income below target) could be true. This example is taken from my household income below target because, it would include income that would allow them to repay this loan or borrow money to pay rent to someone within my house.\n",
            "Although living in a very poor housing scenario, this example is likely true. However, if someone live in your home, rent is an expense so pay rent to someone within your household. Although this example (from my household income below target) could be true, this situation is similar to a situation that is similar to a situation that is similar to a situation of poor housing (or poverty). In this example, rent is an expense so pay rent to someone within your household.\n",
            "If your living situation is similar to that of poverty, or living situation different from a situation of poverty, then your rent is an expense that is used to pay rent to an individual within your household.\n",
            "I recommend that, when choosing an income income based on income levels, look at their needs, whether they need to be covered with rent (as opposed to rent), and make sure this is the case before making decisions concerning income.\n",
            "In this example, from my household income below target, it would exclude expenses such as food, food stamps, medical care, insurance services, etc. as income.\n",
            "\n",
            "[100 | 104.96] loss=3.80 avg=3.73\n",
            "[101 | 105.90] loss=4.35 avg=3.74\n",
            "[102 | 106.83] loss=4.13 avg=3.74\n",
            "[103 | 107.75] loss=4.13 avg=3.75\n",
            "[104 | 108.67] loss=4.21 avg=3.76\n",
            "[105 | 109.59] loss=3.73 avg=3.76\n",
            "[106 | 110.51] loss=2.92 avg=3.74\n",
            "[107 | 111.44] loss=4.08 avg=3.75\n",
            "[108 | 112.35] loss=4.14 avg=3.76\n",
            "[109 | 113.26] loss=3.82 avg=3.76\n",
            "[110 | 114.18] loss=2.40 avg=3.74\n",
            "[111 | 115.09] loss=3.25 avg=3.73\n",
            "[112 | 116.00] loss=4.04 avg=3.73\n",
            "[113 | 116.90] loss=4.06 avg=3.74\n",
            "[114 | 117.81] loss=2.68 avg=3.72\n",
            "[115 | 118.71] loss=4.37 avg=3.73\n",
            "[116 | 119.62] loss=3.89 avg=3.73\n",
            "[117 | 120.52] loss=4.17 avg=3.74\n",
            "[118 | 121.42] loss=4.13 avg=3.75\n",
            "[119 | 122.32] loss=3.97 avg=3.75\n",
            "[120 | 123.22] loss=2.84 avg=3.74\n",
            "[121 | 124.12] loss=4.33 avg=3.75\n",
            "[122 | 125.01] loss=3.93 avg=3.75\n",
            "[123 | 125.91] loss=4.51 avg=3.76\n",
            "[124 | 126.81] loss=3.91 avg=3.76\n",
            "[125 | 127.71] loss=3.52 avg=3.76\n",
            "[126 | 128.61] loss=2.85 avg=3.74\n",
            "[127 | 129.50] loss=4.27 avg=3.75\n",
            "[128 | 130.40] loss=3.84 avg=3.75\n",
            "[129 | 131.29] loss=4.03 avg=3.76\n",
            "[130 | 132.18] loss=4.18 avg=3.76\n",
            "[131 | 133.07] loss=4.14 avg=3.77\n",
            "[132 | 133.96] loss=4.26 avg=3.77\n",
            "[133 | 134.85] loss=4.34 avg=3.78\n",
            "[134 | 135.75] loss=3.86 avg=3.78\n",
            "[135 | 136.64] loss=4.38 avg=3.79\n",
            "[136 | 137.53] loss=4.16 avg=3.80\n",
            "[137 | 138.42] loss=4.05 avg=3.80\n",
            "[138 | 139.31] loss=4.14 avg=3.80\n",
            "[139 | 140.21] loss=4.15 avg=3.81\n",
            "[140 | 141.10] loss=2.66 avg=3.79\n",
            "[141 | 141.99] loss=3.82 avg=3.79\n",
            "[142 | 142.88] loss=4.15 avg=3.80\n",
            "[143 | 143.77] loss=4.16 avg=3.80\n",
            "[144 | 144.67] loss=3.94 avg=3.81\n",
            "[145 | 145.56] loss=4.01 avg=3.81\n",
            "[146 | 146.45] loss=4.01 avg=3.81\n",
            "[147 | 147.34] loss=4.14 avg=3.82\n",
            "[148 | 148.24] loss=3.63 avg=3.81\n",
            "[149 | 149.13] loss=3.98 avg=3.81\n",
            "[150 | 150.02] loss=4.33 avg=3.82\n",
            "[151 | 150.92] loss=3.93 avg=3.82\n",
            "[152 | 151.81] loss=4.20 avg=3.83\n",
            "[153 | 152.70] loss=3.97 avg=3.83\n",
            "[154 | 153.60] loss=3.93 avg=3.83\n",
            "[155 | 154.49] loss=4.31 avg=3.84\n",
            "[156 | 155.39] loss=2.54 avg=3.82\n",
            "[157 | 156.29] loss=4.07 avg=3.82\n",
            "[158 | 157.19] loss=2.53 avg=3.81\n",
            "[159 | 158.09] loss=3.80 avg=3.81\n",
            "[160 | 158.99] loss=2.60 avg=3.79\n",
            "[161 | 159.89] loss=2.55 avg=3.78\n",
            "[162 | 160.79] loss=3.30 avg=3.77\n",
            "[163 | 161.69] loss=3.71 avg=3.77\n",
            "[164 | 162.60] loss=4.01 avg=3.77\n",
            "[165 | 163.50] loss=4.48 avg=3.78\n",
            "[166 | 164.41] loss=4.05 avg=3.79\n",
            "[167 | 165.31] loss=4.17 avg=3.79\n",
            "[168 | 166.22] loss=3.98 avg=3.79\n",
            "[169 | 167.13] loss=4.08 avg=3.80\n",
            "[170 | 168.03] loss=3.72 avg=3.79\n",
            "[171 | 168.94] loss=4.18 avg=3.80\n",
            "[172 | 169.85] loss=4.15 avg=3.80\n",
            "[173 | 170.76] loss=3.35 avg=3.80\n",
            "[174 | 171.67] loss=3.83 avg=3.80\n",
            "[175 | 172.59] loss=2.63 avg=3.78\n",
            "[176 | 173.50] loss=2.70 avg=3.77\n",
            "[177 | 174.41] loss=3.72 avg=3.77\n",
            "[178 | 175.32] loss=4.14 avg=3.78\n",
            "[179 | 176.23] loss=2.75 avg=3.76\n",
            "[180 | 177.14] loss=4.09 avg=3.77\n",
            "[181 | 178.06] loss=3.62 avg=3.77\n",
            "[182 | 178.97] loss=4.06 avg=3.77\n",
            "[183 | 179.88] loss=3.84 avg=3.77\n",
            "[184 | 180.79] loss=3.82 avg=3.77\n",
            "[185 | 181.71] loss=2.60 avg=3.76\n",
            "[186 | 182.62] loss=3.96 avg=3.76\n",
            "[187 | 183.53] loss=3.97 avg=3.76\n",
            "[188 | 184.44] loss=4.14 avg=3.77\n",
            "[189 | 185.35] loss=3.73 avg=3.77\n",
            "[190 | 186.27] loss=4.12 avg=3.77\n",
            "[191 | 187.18] loss=3.77 avg=3.77\n",
            "[192 | 188.09] loss=4.13 avg=3.77\n",
            "[193 | 189.00] loss=4.02 avg=3.78\n",
            "[194 | 189.91] loss=4.18 avg=3.78\n",
            "[195 | 190.82] loss=2.66 avg=3.77\n",
            "[196 | 191.73] loss=4.29 avg=3.77\n",
            "[197 | 192.64] loss=2.54 avg=3.76\n",
            "[198 | 193.55] loss=2.63 avg=3.75\n",
            "[199 | 194.47] loss=3.81 avg=3.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "alle that is used in these experiments to describe this effect of food content on lipid metabolism:\n",
            "A group of three rats at 1 week feeding, each of which received 2% (5% from protein, 25% from sugar) of food a daily as recommended by their dietitian during the preceding 4 months. Each of these rats were to be examined in order to determine a dietary (protein, carbohydrate) to be added in order to obtain results. During feeding at least 3 days after feeding, four of the rats received 2% (10% from sugar) of food a daily as recommended by their dietitian during the preceding 4 years after feeding. During feeding, these groups of rats were to be examined in order to define a dietary to be added to their diets daily (Figure 1)  from each diet to determine whether (b) these rats were to be treated in this manner or in an -freezer (i.e., no weight gain as determined by a food control group which received no food at all other than sugar) \n",
            "This is a case of a small dose (0.2 ng) of sugar, rather than a dietitian. It is likely that these rats were treated in small doses because of an obvious limitation in this study that these rats, with very little energy, were treated in -freezer in case they were switched to -freezer  at 0.2 ng of sugar to avoid a possible weight gain.\n",
            "However, these rats on an -freeze were fed a diet in case their energy needed were -freeze. -freezer is a high fructose diet with higher energy requirements because it consists of about 30% sucrose, which results in weight gain compared to those in -freeze (5%). They were then switched to another -freeze. -freeze is highly concentrated fructose which is much less concentrated than sucrose in terms of energy requirements. -type diet used in -freeze rats resulted in a high carbohydrate, high fat to very high fat to be avoided in the -freeze rat.\n",
            "This study found that higher weight gain in -freeze rats in case their energy needed were added (Figure 1). This was done in order to reduce weight gain caused by an -freeze as -freezer to only 10 kg in case they were switched into a high fructose diet. This study therefore concluded that it is important to ensure that -based food intake is given to these animals in order to prevent weight gain during -freeze.\n",
            "During feedings, each of these subjects ingested 3 mg sugar a day. This amount of -freezines also consisted of 2 mg sucrose 2.5 mg sucrose -based food, -freezines were also given at 15 g each from each group. These high-yield, high-protein diets were chosen to minimize weight gain caused when -type diets were used in animal training experiments. Also a combination of -freezine and -freezi were added to control to ensure that -type diet  be applied to all subjects in order to prevent weight gain caused by -based food intake intake.\n",
            "There were only four total subjects in this study.\n",
            "In order to determine what effects of diet (diet) on body weight gain to an additional 11, -type diets were used in this study.\n",
            "In addition, total -type diets were also used in this study. These diets contained 1% -type diet, 0% -type diet, -based food, -fed. A similar study reported that there were also effects of -based diet on body weight gain, weight gain caused by this diet.\n",
            "To determine whether different diet intake patterns (diet, type, type- fed) lead to different muscle mass in these -type diets, -type diets were added in conjunction with an -freeze (with a reduction to 10% weight gain of -type diet). In order to ensure that both diets did indeed increase the muscle mass of an individual individual muscle, -type diets were added to animals fed -freeze diets at 1 week feeding to ensure that it decreased muscle mass as well as muscle growth factors.\n",
            "In conclusion, it seems that -based diets provide high benefits in improving weight gain.\n",
            "In order to reduce body fat density, -based diet were added to animals at 12 weeks feeding. To achieve muscle mass, -based diets were added to animals in order to reduce calorie need because -based diets required less calories to achieve muscle mass in order to make this dietary. In addition, -type diets were added to animals in order to reduce caloric expenditure with these animals because -type diets required more calories.\n",
            "Table 2 Weight Gain Weight Gain Weight Gain Total -type (control) Diet (diet 1) 1% - type (control) 1% 2% 3% 4% 5\n",
            "\n",
            "[200 | 210.66] loss=4.33 avg=3.75\n",
            "[201 | 211.56] loss=2.13 avg=3.74\n",
            "[202 | 212.47] loss=3.38 avg=3.73\n",
            "[203 | 213.38] loss=3.32 avg=3.73\n",
            "[204 | 214.28] loss=3.64 avg=3.73\n",
            "[205 | 215.19] loss=4.13 avg=3.73\n",
            "[206 | 216.10] loss=4.21 avg=3.74\n",
            "[207 | 217.01] loss=3.97 avg=3.74\n",
            "[208 | 217.91] loss=4.03 avg=3.74\n",
            "[209 | 218.82] loss=4.02 avg=3.74\n",
            "[210 | 219.72] loss=3.92 avg=3.75\n",
            "[211 | 220.62] loss=3.97 avg=3.75\n",
            "[212 | 221.53] loss=2.51 avg=3.74\n",
            "[213 | 222.43] loss=3.92 avg=3.74\n",
            "[214 | 223.33] loss=2.43 avg=3.72\n",
            "[215 | 224.23] loss=4.41 avg=3.73\n",
            "[216 | 225.14] loss=4.19 avg=3.74\n",
            "[217 | 226.04] loss=3.74 avg=3.74\n",
            "[218 | 226.94] loss=4.25 avg=3.74\n",
            "[219 | 227.84] loss=2.86 avg=3.73\n",
            "[220 | 228.74] loss=4.12 avg=3.74\n",
            "[221 | 229.64] loss=4.11 avg=3.74\n",
            "[222 | 230.55] loss=4.56 avg=3.75\n",
            "[223 | 231.45] loss=4.14 avg=3.75\n",
            "[224 | 232.36] loss=3.00 avg=3.75\n",
            "[225 | 233.26] loss=2.96 avg=3.74\n",
            "[226 | 234.17] loss=2.62 avg=3.72\n",
            "[227 | 235.07] loss=3.89 avg=3.73\n",
            "[228 | 235.97] loss=3.94 avg=3.73\n",
            "[229 | 236.88] loss=2.28 avg=3.71\n",
            "[230 | 237.78] loss=4.04 avg=3.72\n",
            "[231 | 238.68] loss=3.97 avg=3.72\n",
            "[232 | 239.58] loss=4.06 avg=3.72\n",
            "[233 | 240.48] loss=3.89 avg=3.72\n",
            "[234 | 241.39] loss=4.01 avg=3.73\n",
            "[235 | 242.29] loss=4.12 avg=3.73\n",
            "[236 | 243.19] loss=2.77 avg=3.72\n",
            "[237 | 244.10] loss=4.24 avg=3.73\n",
            "[238 | 245.00] loss=2.54 avg=3.71\n",
            "[239 | 245.91] loss=3.76 avg=3.71\n",
            "[240 | 246.81] loss=4.15 avg=3.72\n",
            "[241 | 247.72] loss=4.15 avg=3.72\n",
            "[242 | 248.63] loss=4.38 avg=3.73\n",
            "[243 | 249.54] loss=4.23 avg=3.74\n",
            "[244 | 250.45] loss=2.39 avg=3.72\n",
            "[245 | 251.35] loss=3.93 avg=3.72\n",
            "[246 | 252.25] loss=4.08 avg=3.73\n",
            "[247 | 253.15] loss=3.75 avg=3.73\n",
            "[248 | 254.06] loss=2.77 avg=3.72\n",
            "[249 | 254.96] loss=3.84 avg=3.72\n",
            "[250 | 255.87] loss=2.52 avg=3.71\n",
            "[251 | 256.77] loss=4.29 avg=3.71\n",
            "[252 | 257.68] loss=4.41 avg=3.72\n",
            "[253 | 258.58] loss=2.60 avg=3.71\n",
            "[254 | 259.49] loss=4.14 avg=3.71\n",
            "[255 | 260.40] loss=4.19 avg=3.72\n",
            "[256 | 261.31] loss=4.14 avg=3.72\n",
            "[257 | 262.21] loss=4.27 avg=3.73\n",
            "[258 | 263.11] loss=4.07 avg=3.73\n",
            "[259 | 264.02] loss=3.79 avg=3.73\n",
            "[260 | 264.93] loss=4.21 avg=3.74\n",
            "[261 | 265.84] loss=2.28 avg=3.72\n",
            "[262 | 266.75] loss=1.17 avg=3.69\n",
            "[263 | 267.66] loss=4.39 avg=3.70\n",
            "[264 | 268.57] loss=3.69 avg=3.70\n",
            "[265 | 269.48] loss=2.70 avg=3.69\n",
            "[266 | 270.40] loss=4.09 avg=3.70\n",
            "[267 | 271.30] loss=3.79 avg=3.70\n",
            "[268 | 272.21] loss=3.65 avg=3.70\n",
            "[269 | 273.12] loss=4.09 avg=3.70\n",
            "[270 | 274.03] loss=4.09 avg=3.70\n",
            "[271 | 274.94] loss=3.95 avg=3.71\n",
            "[272 | 275.86] loss=2.65 avg=3.70\n",
            "[273 | 276.77] loss=3.56 avg=3.69\n",
            "[274 | 277.68] loss=4.03 avg=3.70\n",
            "[275 | 278.58] loss=4.15 avg=3.70\n",
            "[276 | 279.49] loss=3.68 avg=3.70\n",
            "[277 | 280.40] loss=3.87 avg=3.70\n",
            "[278 | 281.31] loss=1.16 avg=3.68\n",
            "[279 | 282.22] loss=3.99 avg=3.68\n",
            "[280 | 283.14] loss=4.03 avg=3.68\n",
            "[281 | 284.05] loss=4.20 avg=3.69\n",
            "[282 | 284.96] loss=3.94 avg=3.69\n",
            "[283 | 285.87] loss=3.99 avg=3.70\n",
            "[284 | 286.78] loss=2.37 avg=3.68\n",
            "[285 | 287.69] loss=4.13 avg=3.69\n",
            "[286 | 288.60] loss=4.10 avg=3.69\n",
            "[287 | 289.52] loss=4.00 avg=3.69\n",
            "[288 | 290.43] loss=4.26 avg=3.70\n",
            "[289 | 291.34] loss=2.55 avg=3.69\n",
            "[290 | 292.25] loss=2.73 avg=3.68\n",
            "[291 | 293.16] loss=2.44 avg=3.66\n",
            "[292 | 294.07] loss=3.75 avg=3.67\n",
            "[293 | 294.98] loss=3.92 avg=3.67\n",
            "[294 | 295.89] loss=4.14 avg=3.67\n",
            "[295 | 296.79] loss=4.28 avg=3.68\n",
            "[296 | 297.70] loss=4.09 avg=3.68\n",
            "[297 | 298.61] loss=4.20 avg=3.69\n",
            "[298 | 299.52] loss=3.93 avg=3.69\n",
            "[299 | 300.43] loss=3.97 avg=3.69\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " designed of, by contrast, will enable people to reduce their dependence on food by using technologies that improve food, rather than by using a model similar to that of traditional energy companies. However, food is already being processed much cheaper than it used to - which means there will also be increased opportunities to reduce their reliance on food while enhancing their capacity to invest in innovation. This means that, if we want to reduce food dependence significantly, we must change approach to this problem.\n",
            "A challenge for policymakers in adopting food policy is that we have a number of technologies that might be useful to reduce dependence on food from other services - but those interventions will have to be integrated into food system as it is designed to operate. Given that food is already being processed much cheaper than it used to - making it more efficient in terms of food production (rather than an attempt to reduce dependence on it), we must start at that point to help to implement them.\n",
            "4.3.3.2. Using Food as Sustainable Energy\n",
            "In this chapter, I will discuss some of potential solutions to food poverty poverty. In addition I will discuss ways in which they might be implemented.\n",
            "An example: In 2013, we were introduced to a new type of sustainable energy innovation called Smart Energy. This is an energy innovation that focuses on improving efficiency in renewable energy. A recent project, which included energy generation from geothermal waste basins, started at an energy efficiency that is estimated to significantly reduce dependence on energy generated when used. When applied to renewable technologies, this could have a dramatic effect on food prices by reducing them in order to increase demand.\n",
            "In this chapter, I also outline several potential solutions to food poverty poverty.\n",
            "It is important that we approach food as sustainable energy  as appropriate policy intervention rather than an adaptation policy. With food, we also need to focus on improving it because some of those services provided by technologies that reduce vulnerability to hunger  thus make it more affordable to those people most at risk of hunger. As a example ,   is being proposed as a framework that requires more information about food quality, including access rates.\n",
            "As is the case with many innovative technologies, there have several important implications to consider at this stage:\n",
            "1. Food as Sustainable Energy?\n",
            "While most of food producers have implemented smart energy  policies  policy approaches so to address hunger poverty, no national governments or private sector entities have been able to deliver enough food at high prices.\n",
            "3. Food as Sustainable Energy?\n",
            "Given this complexity in food policy, it should be apparent to many people that it would be best to focus less on specific technologies but more on approaches to reduce dependence on food rather than focusing entirely on food as sustainable energy .\n",
            "2. Food as Sustainable Energy?\n",
            "A better food is made more convenient by reducing dependence.\n",
            "A much better food is made easier by reducing reliance on food from other services.\n",
            "This is especially true if we implement a policy to reduce dependence on food. This would help to minimize food dependence even further by providing consumers with options to consume more food less frequently.\n",
            "A better food is made better by reducing dependence on food from other services. Even if we do adopt a policy that reduces dependence on food from other services, it will at times result in fewer people with access to nutritious food. What this means is that we must help to reduce dependence on food, rather than to provide services that decrease dependency on energy produced by a particular technology. This requires an in-depth understanding of the problems in a policy approach to food poverty.\n",
            "In fact I will explain some of these problems in detail so that policymakers will be able to adopt them from a policy point of view.\n",
            "3. Food as Sustainable Energy?\n",
            "3.1. Managing Food as Sustainable Energy (Chapter 2)\n",
            "Most food systems implement various measures that reduce dependence on food, in order to achieve these goals. Some actions implemented under this umbrella, including more food options (such as energy savings in case of food shortages), improved energy efficiency (which it most likely would be used to reduce dependence on energy generated by food), is needed to ensure that we reduce dependence on food in order to be able to invest more money to help to improve food consumption.\n",
            "3.2. Developing Food System Innovation (Chapter 3)\n",
            "This chapter outlines some potential solutions to food poverty poverty. It makes it clear that if we focus food as sustainable energy  policy, we will be more likely to adopt policies of better food as integrated economic policy that address problems of food as poor food systems. It also makes it clear that even if we adopt a policy, there will still have to be improvement of food services under better food systems.\n",
            "An example: If one or more emerging technologies become necessary tools to enable food production in this country, it becomes easier to focus energy efficiency measures in order to improve productivity of food sector, while at the same time reducing reliance on energy by increasing efficiency on food.\n",
            "This chapter highlights some potential strategies that might be possible to incorporate these technologies under better food systems.\n",
            "These strategies\n",
            "\n",
            "[300 | 316.09] loss=4.15 avg=3.70\n",
            "[301 | 317.00] loss=4.17 avg=3.70\n",
            "[302 | 317.90] loss=4.11 avg=3.71\n",
            "[303 | 318.80] loss=4.31 avg=3.71\n",
            "[304 | 319.71] loss=4.20 avg=3.72\n",
            "[305 | 320.62] loss=3.97 avg=3.72\n",
            "[306 | 321.53] loss=4.17 avg=3.73\n",
            "[307 | 322.44] loss=3.82 avg=3.73\n",
            "[308 | 323.34] loss=4.51 avg=3.74\n",
            "[309 | 324.25] loss=2.41 avg=3.72\n",
            "[310 | 325.16] loss=2.56 avg=3.71\n",
            "[311 | 326.07] loss=2.81 avg=3.70\n",
            "[312 | 326.98] loss=3.96 avg=3.70\n",
            "[313 | 327.88] loss=4.11 avg=3.71\n",
            "[314 | 328.79] loss=3.77 avg=3.71\n",
            "[315 | 329.70] loss=4.11 avg=3.71\n",
            "[316 | 330.61] loss=4.02 avg=3.72\n",
            "[317 | 331.51] loss=2.63 avg=3.70\n",
            "[318 | 332.42] loss=4.05 avg=3.71\n",
            "[319 | 333.33] loss=3.87 avg=3.71\n",
            "[320 | 334.23] loss=3.82 avg=3.71\n",
            "[321 | 335.14] loss=2.48 avg=3.70\n",
            "[322 | 336.05] loss=4.06 avg=3.70\n",
            "[323 | 336.96] loss=3.50 avg=3.70\n",
            "[324 | 337.86] loss=2.38 avg=3.69\n",
            "[325 | 338.77] loss=2.49 avg=3.67\n",
            "[326 | 339.68] loss=4.13 avg=3.68\n",
            "[327 | 340.58] loss=4.15 avg=3.68\n",
            "[328 | 341.48] loss=3.76 avg=3.68\n",
            "[329 | 342.38] loss=4.05 avg=3.69\n",
            "[330 | 343.29] loss=4.03 avg=3.69\n",
            "[331 | 344.20] loss=4.35 avg=3.70\n",
            "[332 | 345.10] loss=3.87 avg=3.70\n",
            "[333 | 346.00] loss=4.35 avg=3.71\n",
            "[334 | 346.91] loss=4.33 avg=3.71\n",
            "[335 | 347.81] loss=4.03 avg=3.72\n",
            "[336 | 348.72] loss=4.19 avg=3.72\n",
            "[337 | 349.63] loss=3.97 avg=3.72\n",
            "[338 | 350.54] loss=2.48 avg=3.71\n",
            "[339 | 351.45] loss=4.09 avg=3.72\n",
            "[340 | 352.35] loss=3.99 avg=3.72\n",
            "[341 | 353.26] loss=3.98 avg=3.72\n",
            "[342 | 354.17] loss=4.27 avg=3.73\n",
            "[343 | 355.08] loss=4.16 avg=3.73\n",
            "[344 | 355.98] loss=3.88 avg=3.73\n",
            "[345 | 356.89] loss=4.15 avg=3.74\n",
            "[346 | 357.80] loss=4.07 avg=3.74\n",
            "[347 | 358.70] loss=4.05 avg=3.74\n",
            "[348 | 359.61] loss=3.82 avg=3.74\n",
            "[349 | 360.51] loss=2.49 avg=3.73\n",
            "[350 | 361.42] loss=3.85 avg=3.73\n",
            "[351 | 362.33] loss=3.92 avg=3.73\n",
            "[352 | 363.24] loss=4.10 avg=3.74\n",
            "[353 | 364.14] loss=2.57 avg=3.73\n",
            "[354 | 365.05] loss=4.01 avg=3.73\n",
            "[355 | 365.95] loss=4.07 avg=3.73\n",
            "[356 | 366.86] loss=4.24 avg=3.74\n",
            "[357 | 367.76] loss=4.07 avg=3.74\n",
            "[358 | 368.67] loss=4.03 avg=3.74\n",
            "[359 | 369.58] loss=4.00 avg=3.75\n",
            "[360 | 370.48] loss=2.77 avg=3.74\n",
            "[361 | 371.38] loss=4.06 avg=3.74\n",
            "[362 | 372.29] loss=4.35 avg=3.75\n",
            "[363 | 373.20] loss=3.89 avg=3.75\n",
            "[364 | 374.11] loss=4.24 avg=3.75\n",
            "[365 | 375.01] loss=2.49 avg=3.74\n",
            "[366 | 375.92] loss=3.70 avg=3.74\n",
            "[367 | 376.82] loss=4.33 avg=3.75\n",
            "[368 | 377.73] loss=3.98 avg=3.75\n",
            "[369 | 378.63] loss=3.79 avg=3.75\n",
            "[370 | 379.53] loss=3.80 avg=3.75\n",
            "[371 | 380.44] loss=3.76 avg=3.75\n",
            "[372 | 381.34] loss=2.72 avg=3.74\n",
            "[373 | 382.25] loss=1.17 avg=3.71\n",
            "[374 | 383.15] loss=3.87 avg=3.71\n",
            "[375 | 384.06] loss=4.10 avg=3.72\n",
            "[376 | 384.96] loss=3.69 avg=3.72\n",
            "[377 | 385.86] loss=3.93 avg=3.72\n",
            "[378 | 386.78] loss=3.94 avg=3.72\n",
            "[379 | 387.68] loss=3.81 avg=3.72\n",
            "[380 | 388.59] loss=3.90 avg=3.72\n",
            "[381 | 389.49] loss=3.61 avg=3.72\n",
            "[382 | 390.40] loss=3.94 avg=3.73\n",
            "[383 | 391.30] loss=4.30 avg=3.73\n",
            "[384 | 392.20] loss=2.29 avg=3.72\n",
            "[385 | 393.10] loss=3.81 avg=3.72\n",
            "[386 | 394.00] loss=4.29 avg=3.72\n",
            "[387 | 394.91] loss=3.83 avg=3.72\n",
            "[388 | 395.82] loss=3.99 avg=3.73\n",
            "[389 | 396.72] loss=4.11 avg=3.73\n",
            "[390 | 397.63] loss=4.07 avg=3.73\n",
            "[391 | 398.54] loss=4.04 avg=3.74\n",
            "[392 | 399.44] loss=3.82 avg=3.74\n",
            "[393 | 400.35] loss=4.22 avg=3.74\n",
            "[394 | 401.26] loss=2.85 avg=3.73\n",
            "[395 | 402.16] loss=3.83 avg=3.74\n",
            "[396 | 403.07] loss=3.92 avg=3.74\n",
            "[397 | 403.97] loss=3.94 avg=3.74\n",
            "[398 | 404.88] loss=2.63 avg=3.73\n",
            "[399 | 405.78] loss=3.86 avg=3.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "(a) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down to a single number. (b) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down to one more number. (c) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down to either greater or lesser number. The rounding shall be done from nearest number of pupils to nearest number of students in (possibility of other students). Such rounding shall be made from right to left of pupils in (possibility of other students). (d) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down to greater or lesser number. (e) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down from opposite number. This does not apply to number of pupils in (possibility of other students). (f) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down from opposite number. This does not apply to number of pupils in (possibility of other students) . (g) If a specified number of pupils be enrolled in (possibility of other students), such number shall be rounded down from opposite number. This does not apply to number of pupils in (possibility of other students) . If any part of a specified number shall fall into place a part in this country, it shall be a part of a rest of country.\n",
            "(h) If a specified number of students be enrolled in (possibility of other students), such number shall be rounded down from opposite number. This does not apply to number of pupils in (possibility of other students) . (i) If a specified number of pupils are enrolled in (possibility of other students), such number is rounded down from opposite number. This does not apply to number of pupils in (possibility of other students) . This does only apply to number of pupils in (possibility of other students). If some part of a specified number shall fall into place a part in this country, it shall be a part of a rest of country.\n",
            "Annotations to this Section\n",
            "A, A, B, D, E, F)\n",
            "1. A student who is under 18 years of age is referred one year to a period of up to five years (n = ) from 18 up to 60 days beginning from 6 months . If \n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )\n",
            "(n = )<|endoftext|>When it comes to developing a game to attract attention to your game-based platform strategy, many people have no concept of what it is they should do.\n",
            "\n",
            "It just makes sense that game developers would do that.\n",
            "\n",
            "So what happens after they have to do it\n",
            "\n",
            "First step in building a game to attract attention is to develop it\n",
            "\n",
            "There is a certain kind of game called an example game, where\n",
            "\n",
            "when it comes to developing a game, most people, even if they know what to\n",
            "do,\n",
            "\n",
            "they have no ideas of what their purpose\n",
            "\n",
            "they have only a theory of game that is\n",
            "\n",
            "procedural basis of game\n",
            "\n",
            "To build a development strategy game requires a\n",
            "\n",
            "considerment that,\n",
            "\n",
            "there\n",
            "\n",
            "have to be some\n",
            "\n",
            "concepts,\n",
            "\n",
            "there\n",
            "\n",
            "how to\n",
            "\n",
            "to develop it.\n",
            "\n",
            "In other words,\n",
            "\n",
            "There\n",
            "\n",
            "also a\n",
            "\n",
            "procedural basis that,\n",
            "\n",
            "there\n",
            "\n",
            "that\n",
            "\n",
            "in\n",
            "\n",
            "different states.\n",
            "\n",
            "There\n",
            "\n",
            "a\n",
            "\n",
            "numbers of things\n",
            "\n",
            "that a\n",
            "\n",
            "numerical\n",
            "\n",
            "solver.\n",
            "\n",
            "This\n",
            "\n",
            "is to\n",
            "\n",
            "find\n",
            "\n",
            "a\n",
            "\n",
            "numbers of\n",
            "\n",
            "things that a\n",
            "\n",
            "numerical\n",
            "\n",
            "solver.\n",
            "\n",
            "What\n",
            "\n",
            "is very easy\n",
            "\n",
            "to do,\n",
            "\n",
            "so\n",
            "\n",
            "there\n",
            "\n",
            "there\n",
            "\n",
            "are some\n",
            "\n",
            "tactics\n",
            "\n",
            "that\n",
            "\n",
            "[400 | 421.18] loss=3.68 avg=3.73\n",
            "[401 | 422.09] loss=3.73 avg=3.73\n",
            "[402 | 423.00] loss=3.88 avg=3.73\n",
            "[403 | 423.91] loss=4.12 avg=3.73\n",
            "[404 | 424.82] loss=4.05 avg=3.74\n",
            "[405 | 425.72] loss=2.68 avg=3.73\n",
            "[406 | 426.62] loss=4.27 avg=3.73\n",
            "[407 | 427.53] loss=4.07 avg=3.74\n",
            "[408 | 428.44] loss=3.93 avg=3.74\n",
            "[409 | 429.34] loss=3.82 avg=3.74\n",
            "[410 | 430.25] loss=2.33 avg=3.72\n",
            "[411 | 431.16] loss=3.78 avg=3.73\n",
            "[412 | 432.06] loss=4.20 avg=3.73\n",
            "[413 | 432.97] loss=3.69 avg=3.73\n",
            "[414 | 433.87] loss=3.96 avg=3.73\n",
            "[415 | 434.78] loss=4.08 avg=3.74\n",
            "[416 | 435.68] loss=4.07 avg=3.74\n",
            "[417 | 436.59] loss=4.00 avg=3.74\n",
            "[418 | 437.49] loss=4.27 avg=3.75\n",
            "[419 | 438.40] loss=4.05 avg=3.75\n",
            "[420 | 439.31] loss=2.26 avg=3.73\n",
            "[421 | 440.22] loss=4.10 avg=3.74\n",
            "[422 | 441.12] loss=4.13 avg=3.74\n",
            "[423 | 442.03] loss=4.21 avg=3.75\n",
            "[424 | 442.93] loss=4.39 avg=3.75\n",
            "[425 | 443.84] loss=3.92 avg=3.76\n",
            "[426 | 444.75] loss=2.57 avg=3.74\n",
            "[427 | 445.66] loss=1.19 avg=3.72\n",
            "[428 | 446.57] loss=4.33 avg=3.72\n",
            "[429 | 447.48] loss=4.32 avg=3.73\n",
            "[430 | 448.39] loss=3.97 avg=3.73\n",
            "[431 | 449.29] loss=3.84 avg=3.73\n",
            "[432 | 450.20] loss=2.67 avg=3.72\n",
            "[433 | 451.11] loss=3.85 avg=3.72\n",
            "[434 | 452.01] loss=3.94 avg=3.73\n",
            "[435 | 452.91] loss=0.99 avg=3.70\n",
            "[436 | 453.82] loss=2.46 avg=3.69\n",
            "[437 | 454.72] loss=3.96 avg=3.69\n",
            "[438 | 455.63] loss=4.03 avg=3.69\n",
            "[439 | 456.54] loss=4.07 avg=3.70\n",
            "[440 | 457.45] loss=3.66 avg=3.70\n",
            "[441 | 458.37] loss=3.63 avg=3.69\n",
            "[442 | 459.27] loss=2.46 avg=3.68\n",
            "[443 | 460.18] loss=4.21 avg=3.69\n",
            "[444 | 461.08] loss=3.94 avg=3.69\n",
            "[445 | 461.99] loss=2.65 avg=3.68\n",
            "[446 | 462.90] loss=3.96 avg=3.68\n",
            "[447 | 463.82] loss=4.04 avg=3.69\n",
            "[448 | 464.72] loss=4.01 avg=3.69\n",
            "[449 | 465.63] loss=4.07 avg=3.69\n",
            "[450 | 466.54] loss=4.63 avg=3.70\n",
            "[451 | 467.45] loss=3.85 avg=3.70\n",
            "[452 | 468.35] loss=4.39 avg=3.71\n",
            "[453 | 469.26] loss=3.75 avg=3.71\n",
            "[454 | 470.17] loss=3.89 avg=3.71\n",
            "[455 | 471.07] loss=3.95 avg=3.72\n",
            "[456 | 471.99] loss=3.99 avg=3.72\n",
            "[457 | 472.89] loss=3.94 avg=3.72\n",
            "[458 | 473.81] loss=4.03 avg=3.72\n",
            "[459 | 474.72] loss=4.16 avg=3.73\n",
            "[460 | 475.62] loss=4.13 avg=3.73\n",
            "[461 | 476.53] loss=3.86 avg=3.73\n",
            "[462 | 477.44] loss=3.64 avg=3.73\n",
            "[463 | 478.35] loss=4.37 avg=3.74\n",
            "[464 | 479.26] loss=3.98 avg=3.74\n",
            "[465 | 480.17] loss=3.32 avg=3.74\n",
            "[466 | 481.08] loss=2.64 avg=3.73\n",
            "[467 | 481.98] loss=3.55 avg=3.72\n",
            "[468 | 482.89] loss=3.84 avg=3.73\n",
            "[469 | 483.80] loss=3.24 avg=3.72\n",
            "[470 | 484.72] loss=4.01 avg=3.72\n",
            "[471 | 485.63] loss=2.19 avg=3.71\n",
            "[472 | 486.54] loss=2.33 avg=3.69\n",
            "[473 | 487.44] loss=4.23 avg=3.70\n",
            "[474 | 488.35] loss=3.79 avg=3.70\n",
            "[475 | 489.26] loss=4.02 avg=3.70\n",
            "[476 | 490.17] loss=4.14 avg=3.71\n",
            "[477 | 491.08] loss=2.85 avg=3.70\n",
            "[478 | 491.99] loss=4.24 avg=3.70\n",
            "[479 | 492.90] loss=3.89 avg=3.71\n",
            "[480 | 493.81] loss=4.07 avg=3.71\n",
            "[481 | 494.72] loss=4.27 avg=3.72\n",
            "[482 | 495.63] loss=2.63 avg=3.71\n",
            "[483 | 496.54] loss=3.91 avg=3.71\n",
            "[484 | 497.44] loss=4.05 avg=3.71\n",
            "[485 | 498.35] loss=4.06 avg=3.71\n",
            "[486 | 499.26] loss=3.72 avg=3.71\n",
            "[487 | 500.17] loss=3.91 avg=3.72\n",
            "[488 | 501.09] loss=3.56 avg=3.71\n",
            "[489 | 502.00] loss=4.17 avg=3.72\n",
            "[490 | 502.91] loss=4.10 avg=3.72\n",
            "[491 | 503.82] loss=3.89 avg=3.72\n",
            "[492 | 504.73] loss=4.19 avg=3.73\n",
            "[493 | 505.64] loss=3.77 avg=3.73\n",
            "[494 | 506.55] loss=2.38 avg=3.72\n",
            "[495 | 507.47] loss=4.07 avg=3.72\n",
            "[496 | 508.38] loss=2.33 avg=3.71\n",
            "[497 | 509.29] loss=4.23 avg=3.71\n",
            "[498 | 510.20] loss=4.13 avg=3.72\n",
            "[499 | 511.11] loss=4.40 avg=3.72\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " tools that we often face is: (3) . Examples that have been identified include:\n",
            "\n",
            "(1) . Examples that have been identified include:\n",
            "\n",
            "( 2) . Examples that have been identified include:\n",
            "\n",
            "( 3) . Examples that have been identified include:\n",
            "\n",
            "( 4) -Infectious diseases in agriculture \n",
            "This is very important because it means some farms that do have a problem.\n",
            "\n",
            "In order of number of farm outbreaks, be aware that this figure is a ratio of cases to deaths of disease.\n",
            "\n",
            "1) . Examples that have been identified include:\n",
            "\n",
            "(1) . Examples that have been identified include:\n",
            "\n",
            "(2) . Examples that have been identified include:\n",
            "\n",
            "(3) . Examples that have been identified include:\n",
            "\n",
            "(2) . Examples that have been identified include:\n",
            "\n",
            "(2) . Examples that have been identified include:\n",
            "\n",
            "(3) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(3) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "( 5) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(6) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(6) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(6) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(7) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(8) . Examples that have been identified include:\n",
            "\n",
            "(7) . Examples that have been identified include:\n",
            "\n",
            "(7) . Examples that have been identified include:\n",
            "\n",
            "(8) . Examples that have been identified include:\n",
            "\n",
            "(7) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(7) . Examples that have been identified include:\n",
            "\n",
            "(8) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(6) . Examples that have been identified include:\n",
            "\n",
            "(4) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(7) . Examples that have been identified include:\n",
            "\n",
            "(5) . Examples that have been identified include:\n",
            "\n",
            "(\" (8))(5) In order of time of occurrence of\n",
            "\n",
            "(8) If there an outbreak \n",
            "(7) If there an outbreak \n",
            "\n",
            "(5) If there an outbreak \n",
            "\n",
            "(6) If there an outbreak \n",
            "\n",
            "(4) If there an outbreak \n",
            "\n",
            "(7) If there an outbreak \n",
            "\n",
            "(4) If there \n",
            "\n",
            "(3) If there is any outbreaks \n",
            "\n",
            "(3) If there be outbreaks .  a case of\n",
            "\n",
            "(6) If there be outbreaks .  a case of\n",
            "\n",
            "(7) If there be outbreaks .  a case of\n",
            "\n",
            "(7) If there be outbreaks .  a case of\n",
            "\n",
            "(6) If there be outbreaks .  a case of\n",
            "\n",
            "(4) If there be outbreaks<|endoftext|>A former student at Northwestern University, a former president of the Illinois chapter of the National Association of Manufacturers (NAMTA), a former UVA trustee, a former president of the Iowa State Education Association (NSEA) will be charged with sexually assaulting three of his students.\n",
            "\n",
            "According to a report by Northwestern University Lawyer's Committee, Richard Wiebe will be tried in Iowa County District Court on sexual assault charges, while J.R. Davis will face a charge of criminal sexual conduct in connection with forcible touching.\n",
            "\n",
            "Wiebe, 45, is charged in Iowa County Circuit Court with a statutory sexual assault charge stemming from an investigation into Wiesebee University-Drainville, where he began teaching.\n",
            "\n",
            "Wiesebee University-Drainville is a former student at Northwestern University with a history of sexual inappropriate behavior. It also received a degree in Political Science from Iowa State.\n",
            "\n",
            "Wiesebe will face eight years in county court in January 2018, which could very\n",
            "\n",
            "[500 | 526.23] loss=3.84 avg=3.72\n",
            "[501 | 527.13] loss=3.79 avg=3.72\n",
            "[502 | 528.04] loss=2.34 avg=3.71\n",
            "[503 | 528.94] loss=2.65 avg=3.70\n",
            "[504 | 529.85] loss=3.99 avg=3.70\n",
            "[505 | 530.76] loss=4.38 avg=3.71\n",
            "[506 | 531.66] loss=4.18 avg=3.71\n",
            "[507 | 532.57] loss=2.52 avg=3.70\n",
            "[508 | 533.48] loss=3.71 avg=3.70\n",
            "[509 | 534.38] loss=4.18 avg=3.71\n",
            "[510 | 535.28] loss=4.05 avg=3.71\n",
            "[511 | 536.18] loss=4.35 avg=3.72\n",
            "[512 | 537.08] loss=3.89 avg=3.72\n",
            "[513 | 537.98] loss=4.30 avg=3.72\n",
            "[514 | 538.88] loss=4.10 avg=3.73\n",
            "[515 | 539.79] loss=4.22 avg=3.73\n",
            "[516 | 540.68] loss=3.75 avg=3.73\n",
            "[517 | 541.58] loss=3.72 avg=3.73\n",
            "[518 | 542.48] loss=3.50 avg=3.73\n",
            "[519 | 543.38] loss=2.54 avg=3.72\n",
            "[520 | 544.28] loss=4.46 avg=3.73\n",
            "[521 | 545.18] loss=3.81 avg=3.73\n",
            "[522 | 546.09] loss=3.96 avg=3.73\n",
            "[523 | 546.99] loss=4.11 avg=3.73\n",
            "[524 | 547.89] loss=3.40 avg=3.73\n",
            "[525 | 548.79] loss=2.40 avg=3.72\n",
            "[526 | 549.70] loss=2.55 avg=3.71\n",
            "[527 | 550.60] loss=3.84 avg=3.71\n",
            "[528 | 551.50] loss=3.78 avg=3.71\n",
            "[529 | 552.40] loss=3.97 avg=3.71\n",
            "[530 | 553.30] loss=4.10 avg=3.71\n",
            "[531 | 554.21] loss=2.66 avg=3.70\n",
            "[532 | 555.11] loss=3.97 avg=3.71\n",
            "[533 | 556.01] loss=1.33 avg=3.68\n",
            "[534 | 556.92] loss=4.17 avg=3.69\n",
            "[535 | 557.82] loss=3.83 avg=3.69\n",
            "[536 | 558.72] loss=3.91 avg=3.69\n",
            "[537 | 559.63] loss=2.59 avg=3.68\n",
            "[538 | 560.53] loss=3.98 avg=3.68\n",
            "[539 | 561.44] loss=2.48 avg=3.67\n",
            "[540 | 562.34] loss=2.52 avg=3.66\n",
            "[541 | 563.25] loss=3.37 avg=3.66\n",
            "[542 | 564.15] loss=3.94 avg=3.66\n",
            "[543 | 565.06] loss=4.97 avg=3.67\n",
            "[544 | 565.96] loss=2.51 avg=3.66\n",
            "[545 | 566.87] loss=3.94 avg=3.66\n",
            "[546 | 567.77] loss=2.46 avg=3.65\n",
            "[547 | 568.68] loss=3.80 avg=3.65\n",
            "[548 | 569.59] loss=3.70 avg=3.65\n",
            "[549 | 570.49] loss=3.96 avg=3.66\n",
            "[550 | 571.39] loss=4.11 avg=3.66\n",
            "[551 | 572.30] loss=4.00 avg=3.66\n",
            "[552 | 573.21] loss=3.68 avg=3.66\n",
            "[553 | 574.12] loss=2.60 avg=3.65\n",
            "[554 | 575.03] loss=3.37 avg=3.65\n",
            "[555 | 575.93] loss=3.84 avg=3.65\n",
            "[556 | 576.84] loss=3.86 avg=3.65\n",
            "[557 | 577.76] loss=2.77 avg=3.65\n",
            "[558 | 578.66] loss=4.05 avg=3.65\n",
            "[559 | 579.57] loss=4.04 avg=3.65\n",
            "[560 | 580.48] loss=3.64 avg=3.65\n",
            "[561 | 581.39] loss=4.09 avg=3.66\n",
            "[562 | 582.30] loss=4.04 avg=3.66\n",
            "[563 | 583.21] loss=4.04 avg=3.67\n",
            "[564 | 584.11] loss=3.80 avg=3.67\n",
            "[565 | 585.02] loss=4.09 avg=3.67\n",
            "[566 | 585.93] loss=2.53 avg=3.66\n",
            "[567 | 586.84] loss=2.52 avg=3.65\n",
            "[568 | 587.74] loss=3.68 avg=3.65\n",
            "[569 | 588.65] loss=1.10 avg=3.62\n",
            "[570 | 589.56] loss=3.93 avg=3.63\n",
            "[571 | 590.47] loss=2.53 avg=3.62\n",
            "[572 | 591.37] loss=4.00 avg=3.62\n",
            "[573 | 592.28] loss=3.45 avg=3.62\n",
            "[574 | 593.19] loss=3.65 avg=3.62\n",
            "[575 | 594.11] loss=4.10 avg=3.62\n",
            "[576 | 595.01] loss=2.70 avg=3.61\n",
            "[577 | 595.93] loss=3.83 avg=3.62\n",
            "[578 | 596.83] loss=4.15 avg=3.62\n",
            "[579 | 597.74] loss=3.93 avg=3.62\n",
            "[580 | 598.65] loss=4.23 avg=3.63\n",
            "[581 | 599.56] loss=4.15 avg=3.64\n",
            "[582 | 600.47] loss=3.10 avg=3.63\n",
            "[583 | 601.38] loss=4.14 avg=3.63\n",
            "[584 | 602.30] loss=3.77 avg=3.64\n",
            "[585 | 603.21] loss=3.96 avg=3.64\n",
            "[586 | 604.12] loss=3.98 avg=3.64\n",
            "[587 | 605.03] loss=3.89 avg=3.65\n",
            "[588 | 605.94] loss=3.66 avg=3.65\n",
            "[589 | 606.84] loss=4.20 avg=3.65\n",
            "[590 | 607.75] loss=3.87 avg=3.65\n",
            "[591 | 608.67] loss=4.00 avg=3.66\n",
            "[592 | 609.58] loss=3.92 avg=3.66\n",
            "[593 | 610.49] loss=2.80 avg=3.65\n",
            "[594 | 611.41] loss=3.93 avg=3.65\n",
            "[595 | 612.32] loss=2.64 avg=3.64\n",
            "[596 | 613.23] loss=3.93 avg=3.65\n",
            "[597 | 614.14] loss=4.13 avg=3.65\n",
            "[598 | 615.05] loss=3.71 avg=3.65\n",
            "[599 | 615.96] loss=2.55 avg=3.64\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "This is what makes some areas of Indonesia so very diverse.\n",
            "Some areas such as Ipoh,\n",
            "Tonga,\n",
            "Jibar have many cultures\n",
            "While other parts of Indonesia include\n",
            "most of the islands,\n",
            "in southern Indonesia they have no contact\n",
            "with humans.\n",
            "There there are still more than 2000\n",
            "to come.  countries have made concerted efforts to\n",
            "increase their diversity , which  been seen\n",
            "To this end, we want to share stories of\n",
            "those experiences. We encourage to share with people people of other societies (  e\n",
            "they have to come first to be recognised  important.\n",
            "  stories of this have to do with those cultures  \n",
            "people who have moved them from place to place . They \n",
            "people they came from. It is essential that those stories be\n",
            "consequenced.\n",
            "  -world of  -popula-\n",
            "sions of people is that people have been\n",
            "overcome by their cultures, to be integrated into their cultural\n",
            "culture -   have also been to grow up in those countries \n",
            "have been more liberal in their views, in their beliefs.  other\n",
            "cultural groups have been more tolerant . These more liberal\n",
            "than those others were also more tolerant.\n",
            "A study  of  -popula-\n",
            "sions of people found their attitudes towards those\n",
            " -popula-sions were more liberal than \n",
            " -popula-sions in same countries.  same\n",
            " country,  -popula-sions were more liberal in their\n",
            " -popula-sions in same countries.  same countries were more liberal\n",
            " -popula-sions were less tolerant.\n",
            "Anthropologia, 10: 3 , 2004.\n",
            "It is of course possible that cultures have\n",
            "been in conflict as of course they have been\n",
            "in conflict. It is possible that some cultures \n",
            "been less tolerant than others,  they  been\n",
            "more tolerant than those cultures -  countries have been more\n",
            "politically liberal in their attitudes towards\n",
            "they have been more tolerant than those cultures\n",
            "they  less tolerant than those cultures \n",
            "They -popula-sions   -Popula-\n",
            "popula-sions of people at same country more liberal\n",
            " than those -popula-sions of people in same country more\n",
            "relied more on tolerance rather than it  -culture.  countries \n",
            "have been more tolerant. Their tolerance towards them  \n",
            "culture   other groups. It is possible\n",
            "that these cultures have been more tolerant than those -popula\n",
            "sions  other cultures than those countries that they  \n",
            "them tolerant, -popula-sions of people  more tolerant than\n",
            " those cultures - -popula-sions of people  more tolerant\n",
            "than those cultures - -popula-sions of people  more tolerant.\n",
            "These stories help explain  of  -popula-\n",
            "sions, if they be of value to them people.\n",
            "(  -popula-\n",
            "popula-sions of people  more tolerant than \n",
            "populum of people to  -popula-\n",
            "populo-sions,  -popula-sions of people is of significance to\n",
            "everyone in  -popula-sion of people with  -popula\n",
            "popula-sions of people less tolerant than  -popula-sions.\n",
            "-\n",
            ".\n",
            "in  of people there is a clear example\n",
            "of that   -popula-sion of people less intolerant than\n",
            "those  other cultures, is that\n",
            "people on islands have more tolerance than  \n",
            "popula-sions  -popula-sions of people.\n",
            "They -popula-sions   -Popula\n",
            "popula-sions of people   -popula-\n",
            "popula-sions  -popula-sions of people   -popula-sions\n",
            "  -popula-sions of people more tolerant than  -popula\n",
            "popula-sions of people   -popula-sions of people more tolerant than\n",
            "popula-sions from countries  -popula-sions of people. To\n",
            "make that clear  of peoples more tolerant than them \n",
            "popula-sions of people less tolerant than those cultures.\n",
            "It will be interesting to know some of peoples \n",
            "cultural groups at same time,  of countries  -popula-sions\n",
            "other cultures than those cultures that  to them  to\n",
            "them  more tolerant than those cultures  that\n",
            "they  less tolerant than - popula-sions of people   -popula\n",
            "popula-sions of people  more tolerant than those\n",
            "popula-sions in same countries.\n",
            "If cultures have been in wars more so than others\n",
            "in conflict,  they  more tolerant than those cultures\n",
            "that people in same countries that more tolerant than\n",
            "\n",
            "\n",
            "[600 | 631.56] loss=3.99 avg=3.64\n",
            "[601 | 632.47] loss=3.72 avg=3.64\n",
            "[602 | 633.37] loss=4.03 avg=3.65\n",
            "[603 | 634.28] loss=4.18 avg=3.65\n",
            "[604 | 635.18] loss=4.25 avg=3.66\n",
            "[605 | 636.09] loss=2.72 avg=3.65\n",
            "[606 | 636.99] loss=2.55 avg=3.64\n",
            "[607 | 637.89] loss=3.98 avg=3.64\n",
            "[608 | 638.79] loss=2.56 avg=3.63\n",
            "[609 | 639.69] loss=3.71 avg=3.63\n",
            "[610 | 640.60] loss=3.99 avg=3.64\n",
            "[611 | 641.49] loss=4.04 avg=3.64\n",
            "[612 | 642.40] loss=3.57 avg=3.64\n",
            "[613 | 643.30] loss=3.95 avg=3.64\n",
            "[614 | 644.20] loss=4.31 avg=3.65\n",
            "[615 | 645.10] loss=4.19 avg=3.66\n",
            "[616 | 646.00] loss=3.73 avg=3.66\n",
            "[617 | 646.90] loss=3.68 avg=3.66\n",
            "[618 | 647.80] loss=3.69 avg=3.66\n",
            "[619 | 648.70] loss=2.60 avg=3.65\n",
            "[620 | 649.60] loss=4.37 avg=3.65\n",
            "[621 | 650.49] loss=4.06 avg=3.66\n",
            "[622 | 651.39] loss=3.69 avg=3.66\n",
            "[623 | 652.29] loss=4.09 avg=3.66\n",
            "[624 | 653.19] loss=3.56 avg=3.66\n",
            "[625 | 654.09] loss=2.50 avg=3.65\n",
            "[626 | 654.99] loss=2.70 avg=3.64\n",
            "[627 | 655.89] loss=3.93 avg=3.64\n",
            "[628 | 656.78] loss=4.03 avg=3.65\n",
            "[629 | 657.68] loss=2.55 avg=3.64\n",
            "[630 | 658.58] loss=3.86 avg=3.64\n",
            "[631 | 659.49] loss=2.74 avg=3.63\n",
            "[632 | 660.39] loss=3.83 avg=3.63\n",
            "[633 | 661.29] loss=4.05 avg=3.64\n",
            "[634 | 662.20] loss=3.81 avg=3.64\n",
            "[635 | 663.10] loss=2.91 avg=3.63\n",
            "[636 | 664.00] loss=4.57 avg=3.64\n",
            "[637 | 664.90] loss=2.59 avg=3.63\n",
            "[638 | 665.81] loss=4.15 avg=3.63\n",
            "[639 | 666.71] loss=4.17 avg=3.64\n",
            "[640 | 667.61] loss=2.43 avg=3.63\n",
            "[641 | 668.51] loss=4.65 avg=3.64\n",
            "[642 | 669.41] loss=3.73 avg=3.64\n",
            "[643 | 670.32] loss=3.87 avg=3.64\n",
            "[644 | 671.22] loss=3.99 avg=3.64\n",
            "[645 | 672.13] loss=4.11 avg=3.65\n",
            "[646 | 673.03] loss=3.75 avg=3.65\n",
            "[647 | 673.94] loss=4.05 avg=3.65\n",
            "[648 | 674.84] loss=4.22 avg=3.66\n",
            "[649 | 675.75] loss=2.51 avg=3.65\n",
            "[650 | 676.65] loss=3.95 avg=3.65\n",
            "[651 | 677.56] loss=3.76 avg=3.65\n",
            "[652 | 678.47] loss=2.60 avg=3.64\n",
            "[653 | 679.37] loss=3.65 avg=3.64\n",
            "[654 | 680.28] loss=4.39 avg=3.65\n",
            "[655 | 681.18] loss=2.51 avg=3.64\n",
            "[656 | 682.09] loss=2.66 avg=3.63\n",
            "[657 | 683.00] loss=3.84 avg=3.63\n",
            "[658 | 683.91] loss=3.71 avg=3.63\n",
            "[659 | 684.82] loss=2.37 avg=3.62\n",
            "[660 | 685.72] loss=4.04 avg=3.62\n",
            "[661 | 686.64] loss=3.93 avg=3.63\n",
            "[662 | 687.55] loss=3.77 avg=3.63\n",
            "[663 | 688.46] loss=2.31 avg=3.61\n",
            "[664 | 689.37] loss=2.63 avg=3.60\n",
            "[665 | 690.28] loss=3.74 avg=3.61\n",
            "[666 | 691.19] loss=4.05 avg=3.61\n",
            "[667 | 692.09] loss=4.07 avg=3.61\n",
            "[668 | 693.00] loss=4.31 avg=3.62\n",
            "[669 | 693.91] loss=4.30 avg=3.63\n",
            "[670 | 694.82] loss=3.24 avg=3.62\n",
            "[671 | 695.73] loss=2.37 avg=3.61\n",
            "[672 | 696.64] loss=2.40 avg=3.60\n",
            "[673 | 697.56] loss=3.63 avg=3.60\n",
            "[674 | 698.47] loss=3.58 avg=3.60\n",
            "[675 | 699.38] loss=4.11 avg=3.60\n",
            "[676 | 700.29] loss=3.84 avg=3.61\n",
            "[677 | 701.19] loss=3.92 avg=3.61\n",
            "[678 | 702.11] loss=4.06 avg=3.61\n",
            "[679 | 703.02] loss=2.48 avg=3.60\n",
            "[680 | 703.93] loss=3.79 avg=3.60\n",
            "[681 | 704.84] loss=3.79 avg=3.61\n",
            "[682 | 705.75] loss=2.42 avg=3.59\n",
            "[683 | 706.66] loss=3.89 avg=3.60\n",
            "[684 | 707.57] loss=2.55 avg=3.59\n",
            "[685 | 708.49] loss=4.00 avg=3.59\n",
            "[686 | 709.40] loss=3.82 avg=3.59\n",
            "[687 | 710.30] loss=2.10 avg=3.58\n",
            "[688 | 711.22] loss=4.77 avg=3.59\n",
            "[689 | 712.13] loss=4.24 avg=3.60\n",
            "[690 | 713.04] loss=4.16 avg=3.60\n",
            "[691 | 713.95] loss=3.71 avg=3.60\n",
            "[692 | 714.86] loss=3.68 avg=3.60\n",
            "[693 | 715.78] loss=3.67 avg=3.61\n",
            "[694 | 716.69] loss=4.15 avg=3.61\n",
            "[695 | 717.60] loss=2.48 avg=3.60\n",
            "[696 | 718.51] loss=3.73 avg=3.60\n",
            "[697 | 719.42] loss=2.32 avg=3.59\n",
            "[698 | 720.33] loss=3.80 avg=3.59\n",
            "[699 | 721.24] loss=4.11 avg=3.60\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", of which a good deal of attention is devoted toward determining whether a substance of origin, such as honey bees or beeschia, should be used. After first identification of potential honey-containing compounds by various chemical characteristics in honey bees' colonies, the first steps of evaluation of potential honey-containing compounds are determined. Honey bees be divided into five broad orders of honey-containing compounds listed in Table 1:\n",
            "TABLE 1 Honey bees (m) Honey bees (m) Honey bee (m%) Phosphor phosphor Phosphorus (nm)\n",
            "mH.\n",
            "tO (mH) tO (mH) oPhosphor (nm) Phosphorus  .\n",
            "Honey bees at different stages of production undergo phase phase stages of manufacture, when they develop a long chain of chemical components that in turn be used during stage production.\n",
            "Honey bees have several unique chemical characteristics characterized by a characteristic absence of odor, lack of heat, absence of body temperature, very high concentration of inorganic compounds. These characteristics will have a strong influence on their quality according to different measures.\n",
            "Honey colony production in honeybee colonies is made at different stages of production. In these stages, honey bees be divided into honey-bearing animals of each species with different populations, each with more specific characteristics than others. Although bees generally become young in stage, this will depend on various factors, including their ability to keep temperature temperature at least constant.\n",
            "Early stages (incl. 2 to 16 weeks after harvest), honey bees is mainly active during this time, mainly feeding on ground cover. At late stages, they become active during this time by feeding on vegetation or on crops on which they occupy surface of honey hive (e.g., woody or flower beds, where they feed on soil).\n",
            "Honey bees during late phase of production should first be harvested in November. This stage of production is in order of timing dependent upon specific bee population or insect populations as well as in quantity of honey bees present. Depending on time of harvest a variety of pesticides (including glyphosate) must be available to correct problem situations in honey bees.\n",
            "Figure 7: Honey bees at different stages of honey production in honeybee colonies in Germany following harvest. In many of colonies, honey bees become active at early stage during harvesting or when growing season is past. However, at early stages they do survive at large, often lasting several weeks longer than others.\n",
            "Figure 8: Heteroptera that do reach adult stage of adult honey bee colony.\n",
            "Figure 9: Honey bees at different stages of production in fruit. This is an example of a fruit that requires an immature honey bee brood or larvae until adult stage starts showing signs of maturity.\n",
            "Figure 10: Honey bees in fruit.\n",
            "Figure 11: Honey bees in apple.\n",
            "Figure 12: Honey bee brood.\n",
            "Figure 13: Honey bee larvae (e.g., yellow-spotted homing honey bees), in addition to these four different types of honey bees on tree\n",
            "Figure 14: Honey bee larvae at different stages of production in citrus fruit.\n",
            "Honey bees in fruit fruits require specific specific chemical components which they will need at later stage of production in case of fruit production. Honey bees also need specific chemical pesticides (including glyphosate) to correct problems with fruit production.\n",
            "Figure 15: Honey bee bees at different stages of production in honey honey, fruits. This is an example of a fruit that requires an immature honey bee brood or larvae until adult stage starts showing signs of maturity.\n",
            "Figure 16: Honey bees at different stages of development in fruit. This is an example of a fruit that requires an immature honey bee brood or larvae until adult stage starts showing signs of maturity.\n",
            "Figure 17: Humboldous fruit moth (Hemias sp. sp. Bt.) on tree fruit, from which honey bees develop at earlier stages in each stage. This fruit is called a honeybee moth.\n",
            "Honey bee colonies must be carefully maintained in order to minimize damage to crops as well as promote healthy growth of plants in response to environmental stresses in honey bees.\n",
            "Figure 6: Honey bee colony at different stages of honey production in Germany after harvest.\n",
            "Figure 7: Honey bee colonies located in different stages of honey production in Germany following harvest. Honey bee colonies become active when harvest is completed.\n",
            "Honey bee colonies will be laid in a variety of climates in addition to their usual types of honey bees. From start, honey bees will reach adult stage of honey honey. They will start growing at maturity to mature as honey bees. After that, honey bees become active during flowering at time of flowering. By that time, their body temperature will begin to decrease to -60 degrees, whereas temperature in winter to -70 degrees at harvest.\n",
            "A number of important factors controlling honey bee production in honey colonies. Many of these factors include,\n",
            "Bee hive\n",
            "Honey bee production processes\n",
            "Honey bee reproduction system\n",
            "Honey bees do depend on certain other bees to develop\n",
            "Honey bees perform important\n",
            "\n",
            "[700 | 737.18] loss=3.09 avg=3.59\n",
            "[701 | 738.09] loss=4.15 avg=3.60\n",
            "[702 | 739.00] loss=3.99 avg=3.60\n",
            "[703 | 739.91] loss=4.36 avg=3.61\n",
            "[704 | 740.81] loss=3.95 avg=3.61\n",
            "[705 | 741.71] loss=4.07 avg=3.62\n",
            "[706 | 742.62] loss=4.10 avg=3.62\n",
            "[707 | 743.52] loss=3.95 avg=3.62\n",
            "[708 | 744.42] loss=3.50 avg=3.62\n",
            "[709 | 745.33] loss=2.27 avg=3.61\n",
            "[710 | 746.24] loss=3.93 avg=3.61\n",
            "[711 | 747.14] loss=3.58 avg=3.61\n",
            "[712 | 748.04] loss=2.23 avg=3.60\n",
            "[713 | 748.95] loss=3.91 avg=3.60\n",
            "[714 | 749.85] loss=4.38 avg=3.61\n",
            "[715 | 750.75] loss=4.28 avg=3.62\n",
            "[716 | 751.66] loss=3.80 avg=3.62\n",
            "[717 | 752.56] loss=4.15 avg=3.62\n",
            "[718 | 753.46] loss=4.15 avg=3.63\n",
            "[719 | 754.36] loss=3.79 avg=3.63\n",
            "[720 | 755.26] loss=2.54 avg=3.62\n",
            "[721 | 756.16] loss=3.96 avg=3.62\n",
            "[722 | 757.07] loss=3.79 avg=3.62\n",
            "[723 | 757.96] loss=2.77 avg=3.62\n",
            "[724 | 758.87] loss=4.01 avg=3.62\n",
            "[725 | 759.77] loss=3.90 avg=3.62\n",
            "[726 | 760.67] loss=4.07 avg=3.63\n",
            "[727 | 761.57] loss=3.45 avg=3.62\n",
            "[728 | 762.47] loss=3.31 avg=3.62\n",
            "[729 | 763.38] loss=4.02 avg=3.63\n",
            "[730 | 764.28] loss=4.08 avg=3.63\n",
            "[731 | 765.19] loss=3.28 avg=3.63\n",
            "[732 | 766.08] loss=4.04 avg=3.63\n",
            "[733 | 766.99] loss=4.03 avg=3.63\n",
            "[734 | 767.89] loss=3.33 avg=3.63\n",
            "[735 | 768.79] loss=3.93 avg=3.63\n",
            "[736 | 769.69] loss=2.52 avg=3.62\n",
            "[737 | 770.59] loss=3.79 avg=3.62\n",
            "[738 | 771.49] loss=3.93 avg=3.63\n",
            "[739 | 772.40] loss=2.22 avg=3.61\n",
            "[740 | 773.30] loss=4.02 avg=3.62\n",
            "[741 | 774.20] loss=4.20 avg=3.62\n",
            "[742 | 775.10] loss=4.16 avg=3.63\n",
            "[743 | 776.01] loss=2.58 avg=3.62\n",
            "[744 | 776.91] loss=3.87 avg=3.62\n",
            "[745 | 777.81] loss=4.10 avg=3.63\n",
            "[746 | 778.71] loss=4.01 avg=3.63\n",
            "[747 | 779.61] loss=3.60 avg=3.63\n",
            "[748 | 780.52] loss=3.65 avg=3.63\n",
            "[749 | 781.42] loss=3.75 avg=3.63\n",
            "[750 | 782.32] loss=2.62 avg=3.62\n",
            "[751 | 783.23] loss=3.95 avg=3.62\n",
            "[752 | 784.13] loss=3.88 avg=3.63\n",
            "[753 | 785.04] loss=3.92 avg=3.63\n",
            "[754 | 785.95] loss=3.90 avg=3.63\n",
            "[755 | 786.85] loss=3.94 avg=3.64\n",
            "[756 | 787.75] loss=4.01 avg=3.64\n",
            "[757 | 788.66] loss=4.00 avg=3.64\n",
            "[758 | 789.56] loss=2.58 avg=3.63\n",
            "[759 | 790.46] loss=2.42 avg=3.62\n",
            "[760 | 791.36] loss=4.06 avg=3.62\n",
            "[761 | 792.27] loss=3.88 avg=3.63\n",
            "[762 | 793.17] loss=3.86 avg=3.63\n",
            "[763 | 794.07] loss=3.98 avg=3.63\n",
            "[764 | 794.97] loss=3.92 avg=3.64\n",
            "[765 | 795.87] loss=3.58 avg=3.64\n",
            "[766 | 796.78] loss=2.38 avg=3.62\n",
            "[767 | 797.68] loss=3.91 avg=3.63\n",
            "[768 | 798.58] loss=4.01 avg=3.63\n",
            "[769 | 799.48] loss=2.82 avg=3.62\n",
            "[770 | 800.38] loss=3.61 avg=3.62\n",
            "[771 | 801.28] loss=3.92 avg=3.62\n",
            "[772 | 802.19] loss=2.71 avg=3.61\n",
            "[773 | 803.09] loss=2.44 avg=3.60\n",
            "[774 | 804.00] loss=3.86 avg=3.61\n",
            "[775 | 804.90] loss=2.33 avg=3.59\n",
            "[776 | 805.80] loss=3.73 avg=3.59\n",
            "[777 | 806.70] loss=2.63 avg=3.58\n",
            "[778 | 807.61] loss=4.13 avg=3.59\n",
            "[779 | 808.50] loss=3.72 avg=3.59\n",
            "[780 | 809.40] loss=2.64 avg=3.58\n",
            "[781 | 810.30] loss=3.59 avg=3.58\n",
            "[782 | 811.21] loss=4.05 avg=3.59\n",
            "[783 | 812.11] loss=4.30 avg=3.59\n",
            "[784 | 813.01] loss=3.76 avg=3.60\n",
            "[785 | 813.92] loss=4.08 avg=3.60\n",
            "[786 | 814.82] loss=2.68 avg=3.59\n",
            "[787 | 815.73] loss=2.30 avg=3.58\n",
            "[788 | 816.63] loss=3.62 avg=3.58\n",
            "[789 | 817.54] loss=3.87 avg=3.58\n",
            "[790 | 818.44] loss=3.90 avg=3.58\n",
            "[791 | 819.35] loss=3.93 avg=3.59\n",
            "[792 | 820.25] loss=2.25 avg=3.57\n",
            "[793 | 821.16] loss=2.83 avg=3.57\n",
            "[794 | 822.06] loss=3.90 avg=3.57\n",
            "[795 | 822.96] loss=3.18 avg=3.57\n",
            "[796 | 823.86] loss=3.74 avg=3.57\n",
            "[797 | 824.77] loss=2.61 avg=3.56\n",
            "[798 | 825.67] loss=3.84 avg=3.56\n",
            "[799 | 826.57] loss=3.87 avg=3.56\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " or in relation to that of other countries\n",
            "To provide, in accordance with Article 5, a level-appropriate response to those\n",
            "that were involved in this exercise (or their heirs).\n",
            "These instruments will include,  such actions as:\n",
            "- A general overview of steps required to fulfil Article 5\n",
            "to make appropriate efforts to avoid harm to national\n",
            "economic interests to reduce adverse impact of risks in\n",
            "processes\n",
            "- A description of measures undertaken to address impacts from\n",
            "some of these activities (requirements outlined in Annex 1a )\n",
            "To:\n",
            "- Provide technical assistance activities, (within framework of\n",
            "their provisions\n",
            "- Provide technical assistance activities to support activities\n",
            "other actions undertaken in this exercise\n",
            "To:\n",
            "- Provide, in accordance with Article 5, a level-appropriate response\n",
            "to those actions that could have an adverse impact on\n",
            "their economic interests\n",
            "To:\n",
            "- Provide technical assistance activities, (within framework of\n",
            "those activities )\n",
            "To:\n",
            "- Provide technical assistance activities,\n",
            "to enhance economic growth outcomes\n",
            "to ensure full implementation of measures that would\n",
            "be likely to have an adverse impact on competitiveness\n",
            "in response to adverse actions\n",
            "- Further, to provide technical assistance activities to\n",
            "a range of activities to be undertaken within framework of\n",
            "such activities to improve competitiveness by:\n",
            "- Ensure that technical assistance activities support\n",
            " development activities,\n",
            "(within framework of measures undertaken in Appendix\n",
            "to Annex).\n",
            "To:\n",
            "- Provide technical assistance activities, (within framework of\n",
            "- Ensure that technical assistance activities support\n",
            "- Ensure that technical assistance activities support\n",
            "- Ensure that technical assistance activities support\n",
            "- Ensure that technical assistance activities support\n",
            "- Ensure that technical assistance activities support\n",
            "- Ensure that technical assistance activities support\n",
            "to support activities to promote commercialization\n",
            "that have been identified  in Annex to\n",
            "organise activities within framework of those activities.\n",
            "( Annex)\n",
            "( Appendix)\n",
            " to:\n",
            "- Provide technical assistance activities,\n",
            "( within framework of measures undertaken in Appendix\n",
            "to Annex).\n",
            "( appendix)\n",
            "( appendix)\n",
            "( appendix)\n",
            "( Appendix)\n",
            " to:\n",
            "- Provide technical assistance activities,\n",
            "( within framework of measures undertaken in\n",
            "in Appendix,\n",
            "to:\n",
            "- Provide technical assistance activities,\n",
            "( within framework of those activities )\n",
            "To:\n",
            "- Provide technical assistance activities,\n",
            "( within framework of measures undertaken in Appendix\n",
            "to Appendix).\n",
            "Inclusion: ( Annex( )).\n",
            "( Annex)( appendix)\n",
            "to:\n",
            "- Provide technical assistance activities,\n",
            "( within framework of measures undertaken in\n",
            "( appendix)( appendix)\n",
            "( appendix)\n",
            "( appendix)\n",
            "To:\n",
            "- Provide technical assistance activities,\n",
            "( within framework of measures undertaken in\n",
            " ( appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( ,\n",
            "- Provide technical assistance activities,\n",
            "( within framework of measures undertaken in .\n",
            "( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( Appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)( appendix)(\n",
            "\n",
            "[800 | 842.79] loss=2.65 avg=3.56\n",
            "[801 | 843.70] loss=4.10 avg=3.56\n",
            "[802 | 844.60] loss=4.12 avg=3.57\n",
            "[803 | 845.51] loss=4.31 avg=3.57\n",
            "[804 | 846.41] loss=3.75 avg=3.58\n",
            "[805 | 847.31] loss=3.91 avg=3.58\n",
            "[806 | 848.21] loss=3.76 avg=3.58\n",
            "[807 | 849.11] loss=4.08 avg=3.59\n",
            "[808 | 850.02] loss=3.78 avg=3.59\n",
            "[809 | 850.92] loss=3.71 avg=3.59\n",
            "[810 | 851.83] loss=3.60 avg=3.59\n",
            "[811 | 852.73] loss=3.87 avg=3.59\n",
            "[812 | 853.64] loss=4.13 avg=3.60\n",
            "[813 | 854.55] loss=4.32 avg=3.60\n",
            "[814 | 855.45] loss=3.68 avg=3.61\n",
            "[815 | 856.35] loss=3.85 avg=3.61\n",
            "[816 | 857.26] loss=2.79 avg=3.60\n",
            "[817 | 858.16] loss=3.71 avg=3.60\n",
            "[818 | 859.06] loss=2.49 avg=3.59\n",
            "[819 | 859.97] loss=2.51 avg=3.58\n",
            "[820 | 860.87] loss=4.06 avg=3.58\n",
            "[821 | 861.77] loss=4.19 avg=3.59\n",
            "[822 | 862.67] loss=4.02 avg=3.59\n",
            "[823 | 863.57] loss=2.53 avg=3.58\n",
            "[824 | 864.48] loss=4.58 avg=3.59\n",
            "[825 | 865.38] loss=3.86 avg=3.60\n",
            "[826 | 866.28] loss=2.64 avg=3.59\n",
            "[827 | 867.18] loss=2.30 avg=3.57\n",
            "[828 | 868.09] loss=3.90 avg=3.58\n",
            "[829 | 868.99] loss=4.10 avg=3.58\n",
            "[830 | 869.90] loss=4.12 avg=3.59\n",
            "[831 | 870.80] loss=3.47 avg=3.59\n",
            "[832 | 871.70] loss=3.93 avg=3.59\n",
            "[833 | 872.61] loss=3.95 avg=3.59\n",
            "[834 | 873.51] loss=3.88 avg=3.60\n",
            "[835 | 874.41] loss=4.31 avg=3.60\n",
            "[836 | 875.31] loss=4.13 avg=3.61\n",
            "[837 | 876.21] loss=2.12 avg=3.59\n",
            "[838 | 877.12] loss=3.96 avg=3.60\n",
            "[839 | 878.03] loss=3.97 avg=3.60\n",
            "[840 | 878.93] loss=3.83 avg=3.60\n",
            "[841 | 879.83] loss=4.03 avg=3.61\n",
            "[842 | 880.73] loss=4.30 avg=3.61\n",
            "[843 | 881.63] loss=4.19 avg=3.62\n",
            "[844 | 882.53] loss=3.89 avg=3.62\n",
            "[845 | 883.44] loss=3.85 avg=3.63\n",
            "[846 | 884.34] loss=3.84 avg=3.63\n",
            "[847 | 885.25] loss=4.09 avg=3.63\n",
            "[848 | 886.15] loss=4.14 avg=3.64\n",
            "[849 | 887.06] loss=2.45 avg=3.63\n",
            "[850 | 887.96] loss=3.93 avg=3.63\n",
            "[851 | 888.87] loss=3.73 avg=3.63\n",
            "[852 | 889.77] loss=3.74 avg=3.63\n",
            "[853 | 890.68] loss=3.73 avg=3.63\n",
            "[854 | 891.59] loss=2.34 avg=3.62\n",
            "[855 | 892.49] loss=2.51 avg=3.61\n",
            "[856 | 893.40] loss=2.38 avg=3.60\n",
            "[857 | 894.30] loss=2.34 avg=3.58\n",
            "[858 | 895.20] loss=2.37 avg=3.57\n",
            "[859 | 896.10] loss=4.16 avg=3.58\n",
            "[860 | 897.00] loss=3.83 avg=3.58\n",
            "[861 | 897.91] loss=3.96 avg=3.58\n",
            "[862 | 898.81] loss=3.86 avg=3.59\n",
            "[863 | 899.72] loss=2.64 avg=3.58\n",
            "[864 | 900.62] loss=3.48 avg=3.58\n",
            "[865 | 901.53] loss=4.18 avg=3.58\n",
            "[866 | 902.43] loss=2.64 avg=3.57\n",
            "[867 | 903.34] loss=3.91 avg=3.58\n",
            "[868 | 904.24] loss=3.84 avg=3.58\n",
            "[869 | 905.15] loss=3.80 avg=3.58\n",
            "[870 | 906.05] loss=2.37 avg=3.57\n",
            "[871 | 906.96] loss=3.89 avg=3.57\n",
            "[872 | 907.86] loss=3.39 avg=3.57\n",
            "[873 | 908.77] loss=3.60 avg=3.57\n",
            "[874 | 909.68] loss=3.72 avg=3.57\n",
            "[875 | 910.58] loss=4.06 avg=3.58\n",
            "[876 | 911.49] loss=4.11 avg=3.58\n",
            "[877 | 912.40] loss=2.41 avg=3.57\n",
            "[878 | 913.31] loss=3.75 avg=3.57\n",
            "[879 | 914.21] loss=4.11 avg=3.58\n",
            "[880 | 915.11] loss=3.76 avg=3.58\n",
            "[881 | 916.02] loss=2.48 avg=3.57\n",
            "[882 | 916.92] loss=2.58 avg=3.56\n",
            "[883 | 917.82] loss=2.57 avg=3.55\n",
            "[884 | 918.73] loss=3.84 avg=3.55\n",
            "[885 | 919.64] loss=2.33 avg=3.54\n",
            "[886 | 920.54] loss=3.98 avg=3.54\n",
            "[887 | 921.45] loss=3.53 avg=3.54\n",
            "[888 | 922.36] loss=4.10 avg=3.55\n",
            "[889 | 923.28] loss=3.81 avg=3.55\n",
            "[890 | 924.18] loss=4.01 avg=3.56\n",
            "[891 | 925.08] loss=2.42 avg=3.54\n",
            "[892 | 925.99] loss=2.18 avg=3.53\n",
            "[893 | 926.90] loss=4.24 avg=3.54\n",
            "[894 | 927.80] loss=4.05 avg=3.54\n",
            "[895 | 928.70] loss=3.39 avg=3.54\n",
            "[896 | 929.61] loss=3.82 avg=3.54\n",
            "[897 | 930.52] loss=3.98 avg=3.55\n",
            "[898 | 931.43] loss=3.60 avg=3.55\n",
            "[899 | 932.33] loss=2.70 avg=3.54\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " sa \n",
            "to  or  or to  (   )\n",
            "with or .    \n",
            "or or to to to to to to\n",
            "\n",
            "or to to to to to to to to\n",
            "\n",
            "to  to  .  to \n",
            "to to to to to to to to to\n",
            "\n",
            "or :\n",
            "\n",
            "to or to \n",
            "\n",
            "to to to  , even to\n",
            "\n",
            "Toor \n",
            "\n",
            "to .\n",
            "\n",
            " to  .\n",
            "\n",
            "to or \n",
            "\n",
            "to .\n",
            "\n",
            "to  .\n",
            "\n",
            "to   .\n",
            "\n",
            "to   .\n",
            "\n",
            " to or\n",
            "\n",
            "to  .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to  .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            " to or to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            " Toor .\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "ta :\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "ta :\n",
            "\n",
            "Toor .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "Toor \n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            " to :\n",
            "\n",
            "Toor \n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            " Toor \n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to:\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to and a\n",
            "\n",
            "to\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "Toor \n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            " to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            " to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to or . .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to . .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to . .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "Toor \n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "Toor  * ( )\n",
            "\n",
            "to or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "Toor \n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to . .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "Toor .\n",
            "\n",
            "\n",
            "so\n",
            "to .\n",
            "\n",
            "\n",
            "so or\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "so or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to  .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to . >\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "Toor \n",
            "\n",
            "to . >  . >\n",
            "\n",
            "to Or .\n",
            "\n",
            "to Or .\n",
            "\n",
            "to Or .\n",
            "\n",
            "\n",
            " Toor Or\n",
            "\n",
            "to . >  . >\n",
            "\n",
            "to . (or)\n",
            "Toor \n",
            "to . (or)\n",
            "to . Or\n",
            "\n",
            "to .> or :\n",
            "\n",
            "to  or :\n",
            "\n",
            "to . >  . >\n",
            "\n",
            "to . >\n",
            "\n",
            "to Or .\n",
            "\n",
            "to Or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to . >\n",
            "\n",
            "or or or .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "to .\n",
            "\n",
            "\n",
            "to :\n",
            "\n",
            "to . >\n",
            "\n",
            "to\n",
            "\n",
            "[900 | 949.03] loss=3.83 avg=3.54\n",
            "[901 | 949.94] loss=3.91 avg=3.55\n",
            "[902 | 950.83] loss=4.48 avg=3.56\n",
            "[903 | 951.74] loss=3.64 avg=3.56\n",
            "[904 | 952.64] loss=2.89 avg=3.55\n",
            "[905 | 953.55] loss=4.11 avg=3.56\n",
            "[906 | 954.46] loss=3.91 avg=3.56\n",
            "[907 | 955.36] loss=3.83 avg=3.56\n",
            "[908 | 956.27] loss=4.07 avg=3.57\n",
            "[909 | 957.17] loss=3.89 avg=3.57\n",
            "[910 | 958.08] loss=3.85 avg=3.57\n",
            "[911 | 958.98] loss=3.86 avg=3.58\n",
            "[912 | 959.89] loss=2.23 avg=3.56\n",
            "[913 | 960.80] loss=4.17 avg=3.57\n",
            "[914 | 961.70] loss=4.16 avg=3.57\n",
            "[915 | 962.61] loss=4.00 avg=3.58\n",
            "[916 | 963.51] loss=3.67 avg=3.58\n",
            "[917 | 964.41] loss=2.56 avg=3.57\n",
            "[918 | 965.32] loss=3.96 avg=3.57\n",
            "[919 | 966.23] loss=3.78 avg=3.58\n",
            "[920 | 967.13] loss=3.71 avg=3.58\n",
            "[921 | 968.03] loss=4.87 avg=3.59\n",
            "[922 | 968.94] loss=2.36 avg=3.58\n",
            "[923 | 969.85] loss=3.95 avg=3.58\n",
            "[924 | 970.76] loss=4.06 avg=3.59\n",
            "[925 | 971.66] loss=2.47 avg=3.58\n",
            "[926 | 972.57] loss=3.87 avg=3.58\n",
            "[927 | 973.47] loss=4.77 avg=3.59\n",
            "[928 | 974.38] loss=3.94 avg=3.59\n",
            "[929 | 975.29] loss=4.09 avg=3.60\n",
            "[930 | 976.20] loss=4.06 avg=3.60\n",
            "[931 | 977.11] loss=3.95 avg=3.61\n",
            "[932 | 978.02] loss=3.77 avg=3.61\n",
            "[933 | 978.92] loss=3.59 avg=3.61\n",
            "[934 | 979.83] loss=3.71 avg=3.61\n",
            "[935 | 980.73] loss=2.68 avg=3.60\n",
            "[936 | 981.64] loss=2.08 avg=3.58\n",
            "[937 | 982.54] loss=2.30 avg=3.57\n",
            "[938 | 983.45] loss=4.11 avg=3.58\n",
            "[939 | 984.36] loss=4.12 avg=3.58\n",
            "[940 | 985.27] loss=3.95 avg=3.59\n",
            "[941 | 986.17] loss=3.53 avg=3.59\n",
            "[942 | 987.08] loss=3.84 avg=3.59\n",
            "[943 | 987.99] loss=4.00 avg=3.59\n",
            "[944 | 988.89] loss=2.50 avg=3.58\n",
            "[945 | 989.80] loss=3.69 avg=3.58\n",
            "[946 | 990.71] loss=3.85 avg=3.59\n",
            "[947 | 991.62] loss=3.99 avg=3.59\n",
            "[948 | 992.52] loss=3.96 avg=3.59\n",
            "[949 | 993.44] loss=2.47 avg=3.58\n",
            "[950 | 994.34] loss=4.19 avg=3.59\n",
            "[951 | 995.25] loss=2.66 avg=3.58\n",
            "[952 | 996.15] loss=2.37 avg=3.57\n",
            "[953 | 997.06] loss=3.03 avg=3.56\n",
            "[954 | 997.98] loss=3.58 avg=3.56\n",
            "[955 | 998.89] loss=4.05 avg=3.57\n",
            "[956 | 999.80] loss=3.83 avg=3.57\n",
            "[957 | 1000.71] loss=4.01 avg=3.57\n",
            "[958 | 1001.61] loss=4.22 avg=3.58\n",
            "[959 | 1002.51] loss=3.53 avg=3.58\n",
            "[960 | 1003.42] loss=4.10 avg=3.58\n",
            "[961 | 1004.33] loss=3.46 avg=3.58\n",
            "[962 | 1005.24] loss=4.05 avg=3.59\n",
            "[963 | 1006.15] loss=4.07 avg=3.59\n",
            "[964 | 1007.06] loss=4.05 avg=3.60\n",
            "[965 | 1007.97] loss=3.86 avg=3.60\n",
            "[966 | 1008.87] loss=3.88 avg=3.60\n",
            "[967 | 1009.78] loss=3.84 avg=3.60\n",
            "[968 | 1010.70] loss=3.89 avg=3.61\n",
            "[969 | 1011.60] loss=3.30 avg=3.60\n",
            "[970 | 1012.51] loss=3.72 avg=3.61\n",
            "[971 | 1013.42] loss=4.13 avg=3.61\n",
            "[972 | 1014.33] loss=3.76 avg=3.61\n",
            "[973 | 1015.24] loss=3.49 avg=3.61\n",
            "[974 | 1016.14] loss=3.89 avg=3.61\n",
            "[975 | 1017.05] loss=2.83 avg=3.61\n",
            "[976 | 1017.96] loss=0.97 avg=3.58\n",
            "[977 | 1018.86] loss=3.91 avg=3.58\n",
            "[978 | 1019.77] loss=3.91 avg=3.59\n",
            "[979 | 1020.68] loss=2.36 avg=3.57\n",
            "[980 | 1021.60] loss=4.05 avg=3.58\n",
            "[981 | 1022.51] loss=3.90 avg=3.58\n",
            "[982 | 1023.41] loss=3.80 avg=3.58\n",
            "[983 | 1024.33] loss=4.03 avg=3.59\n",
            "[984 | 1025.24] loss=3.63 avg=3.59\n",
            "[985 | 1026.14] loss=1.17 avg=3.56\n",
            "[986 | 1027.05] loss=4.25 avg=3.57\n",
            "[987 | 1027.96] loss=4.05 avg=3.58\n",
            "[988 | 1028.87] loss=3.80 avg=3.58\n",
            "[989 | 1029.77] loss=3.80 avg=3.58\n",
            "[990 | 1030.69] loss=3.47 avg=3.58\n",
            "[991 | 1031.60] loss=4.09 avg=3.58\n",
            "[992 | 1032.50] loss=4.09 avg=3.59\n",
            "[993 | 1033.41] loss=3.87 avg=3.59\n",
            "[994 | 1034.32] loss=4.07 avg=3.60\n",
            "[995 | 1035.23] loss=3.91 avg=3.60\n",
            "[996 | 1036.14] loss=2.81 avg=3.59\n",
            "[997 | 1037.05] loss=3.75 avg=3.59\n",
            "[998 | 1037.96] loss=3.87 avg=3.60\n",
            "[999 | 1038.87] loss=3.75 avg=3.60\n",
            "Saving checkpoint/agbrain/model-1000\n",
            "Generating samples...\n",
            "interrupted\n",
            "Saving checkpoint/agbrain/model-1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./models/124M/encoder.json ./checkpoint/agbrain/\n",
        "!cp ./models/124M/hparams.json ./checkpoint/agbrain/\n",
        "!cp ./models/124M/vocab.bpe ./checkpoint/agbrain/"
      ],
      "metadata": {
        "id": "rScCGpjDRdfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir ../../drive/MyDrive/agribrain/nshepperd/\n",
        "!cp -r ./checkpoint/agbrain ../../drive/MyDrive/agribrain/nshepperd/"
      ],
      "metadata": {
        "id": "6xvO1H4IYccL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrBjQAwLiUWF",
        "outputId": "f4b1d808-8574-4774-bbde-1ef754267658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accumulate.py  generate_unconditional_samples.py   models\ttrain.py\n",
            "checkpoint     interactive_conditional_samples.py  __pycache__\ttwremat.py\n",
            "corpus.npz     load_dataset.py\t\t\t   sample.py\n",
            "encode.py      memory_saving_gradients.py\t   samples\n",
            "encoder.py     model.py\t\t\t\t   tfremat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./checkpoint/agbrain ./models/"
      ],
      "metadata": {
        "id": "etsOLZ2Qphbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 generate_unconditional_samples.py --model_name agbrain --nsamples 1 --batch_size 1"
      ],
      "metadata": {
        "id": "cqvRxXGLp4Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 generate_unconditional_samples.py --model_name agbrain --nsamples 1 --length 50"
      ],
      "metadata": {
        "id": "rg84dT7dKYR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8f5d40-a264-4673-a4e1-20e06b2fd27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-27 08:16:04.878398: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Water Year: Most of Year. Temperature: Water Year (AIR). \n",
            "temperatures Climate Change (CO2)-Thermal Characteristics Change in equatorial equatorial climate,  \n",
            "other climate. In bad is sunny weather in warm\n"
          ]
        }
      ]
    }
  ]
}